
\documentclass[12pt]{article}

% Packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}

\geometry{a4paper, margin=1in}

% Title and Author
\title{Dynamic Sunrise}
\author{James Thompson}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
The idea of Dynamic sunrise is to use a dynamic ensemble of SAC actors. The dynamic nature of the ensemble will help in two aspects by speeding up exploration early on in the training as well as allowing the ensemble to "prune" ineffective base learners. Furthermore it can help the ensemble learn and adapt to a changing environment. This is an extension to Sunrise which itself is an ensemble method which supports using SAC actors as the base learners.

Here you will find a introduction of the Sunrise algorithm and the ideas that would make up the DSunrise algorithm.

\section{The Sunrise Algorithm}

The Sunrise is a algorithm framework for ensemble learning. For simplicity we will assume Soft Actor Critic (SAC) are used as the base learners. Then Sunrise does three interesting things to make the ensemble works. These are:
\begin{itemize}
    \item \textbf{Weighted bellman backups} - This is where the strength of the update to the actors and the critics is weighted by the standard deviation of the ensemble of the target critic estimates for a given state. This means that situation where the critic agree more will have a stronger update in line with the intuition that it is easier to agree on the truth (correct value).
    \item \textbf{Random initialization and training instances} - To increase diversity of the ensemble of base learners each base learner is randomly initialized and trained on a random subset of the replay buffer. The random subset of the replay buffer is achieved by having each transition given a mask that indicates which base learner can use it to train with.
    \item \textbf{UCB exploration} - To improve the exploration of the agent one can use the uncertainty of the ensemble to guide actions into unexplored areas. In practice this means at each time step choosing the action from the ensemble that maximizes both the expected reward and the standard deviation of the ensemble's value predictions.
\end{itemize}

Furthermore because of the nature of the ensemble it is possible that the ensemble can be parallelized which can result in a wall clock time comparable to single learner algorithms.

\section{The Dsunrise Algorithm}

The DSunrise algorithm is the same as the sunrise algorithm with the addition of a dynamic ensemble. That is on a regular interval (say every 10 episodes) the ensemble is check to see if any of the base learners can be removed. On removal of a base learner a new learner will be instantiated and added to the ensemble. Therefore the two important aspects of the DSunrise algorithm is the removal check and the instantiation of new base learners.

\subsection{Removal Check}

A removal check is a routine that is run on some regular interval (every step, episode, n episodes etc). As there is no managing critic the check only has the current base learners and the historic performance of these base learners to look at. From this information two classes of metrics can be created one is a performance check and the second is a diversity check.

The performance check can either look at historic performance and/or the predicted performance of the base learner. Historic performance could involve a type of time weighted average from the last $n$ episodes. The predicted performance could be the average expected return from a sample from the replay buffer. One could either remove base learners that are below a certain threshold or simply remove the $l$ poorest performing base learners.

The diversity check is trying to quantify how different the base learners are from each other. As the goal of an ensemble is that the base learners are different from each other removing base learners that happen to be too similar to each other will reduce the computational cost of the ensemble and provide opportunity for new base learners to be added to increase diversity. The diversity of a base learners can be understood by either how similar the critics are or how similar the proposed actions are from the polices. Furthermore as there is a unique optimal policy the ensemble of base learners should converge to this optimal policy and so the size of the ensemble will naturally decrease as training progresses.

In practice it is likely that a combination of performance metric and diversity metric would result in the best results. This could look something like


\begin{align*}
    \text{performance}_i^{(t)} &= \sum_{\tau=t-n}^{t} \omega_\tau \cdot R_i^{(\tau)} 
    \quad \text{where} \quad 
    \omega_\tau = \frac{\gamma^{t-\tau}}{\sum_{k=t-n}^{t} \gamma^{t-k}} \\
    \text{diversity}_i^{(t)} &= \frac{1}{|\mathcal{S}| \cdot (|\mathcal{E}|-1)} 
    \sum_{s \in \mathcal{S}} \sum_{\substack{j=1 \\ j \neq i}}^{|\mathcal{E}|} 
    \left| \bm{a}_i(s) - \bm{a}_j(s) \right|_2
\end{align*}


Where:
\begin{itemize}
    \item $\text{performance}_i^{(t)}$: Performance metric for learner $i$ at episode $t$
    \item $\text{diversity}_i^{(t)}$: Diversity metric for learner $i$ at episode $t$
    \item $R_i^{(\tau)}$: Return of learner $i$ in episode $\tau$
    \item $\gamma$: Temporal discount factor ($0.9 \leq \gamma \leq 0.99$)
    \item $n$: Window size for performance evaluation
    \item $\mathcal{S}$: State sample from replay buffer ($|\mathcal{S}| = m$)
    \item $\mathcal{E}$: Current ensemble of learners
    \item $\bm{a}_i(s)$: Action vector of learner $i$ in state $s$
\end{itemize}

The removal decision combines both metrics:
\begin{equation*}
    \text{remove}_i^{(t)} = 
    \begin{cases} 
        1 & \text{if } \dfrac{\text{performance}_i^{(t)}}{\mu_P^{(t)}} < \theta_P 
        \;\text{and}\; \dfrac{\text{diversity}_i^{(t)}}{\mu_D^{(t)}} < \theta_D \\
        1 & \text{if } \dfrac{\text{performance}_i^{(t)}}{\mu_P^{(t)}} < \theta_{\text{crit}} \\
        0 & \text{otherwise}
    \end{cases}
\end{equation*}

With:
\begin{itemize}
    \item $\mu_P^{(t)} = \frac{1}{|\mathcal{E}|} \sum_{k=1}^{|\mathcal{E}|} \text{performance}_k^{(t)}$
    \item $\mu_D^{(t)} = \frac{1}{|\mathcal{E}|} \sum_{k=1}^{|\mathcal{E}|} \text{diversity}_k^{(t)}$
    \item $\theta_P$: Performance threshold ($\approx 0.7$)
    \item $\theta_D$: Diversity threshold ($\approx 0.6$)
    \item $\theta_{\text{crit}}$: Critical performance threshold ($\approx 0.5$)
\end{itemize}



\subsection{Instantiation of new base learners}

When a base learner is removed from the ensemble a new base learner can be instantiated. There are many different ways to instantiate new models.

One way of instantiating a new base learner is to randomly instantiate a new base learner then train it up on some random subset of the replay buffer. This will add variation to it through the random initialization and the random subset of the replay buffer. However depending on the size of the experience subset this could be computationally similar to simply having the base learner as part of the ensemble since the beginning of the training.

Another way to instantiate a new base learner is for it to somehow be a variation of a current base learner. This could be done in a variety of ways which can broadly be separated into either cross over (involving multiple base learners) or mutation (involving a single base learner).

Once the instantiation of the new base learner is complete it could pass through a removal check to see if it is worth keeping in the ensemble. If it is not worth keeping then the ensemble size will naturally decrease. This naturally decreasing ensemble size is a feature that trends towards a single base learner which is computationally efficient (and potentially the optimal policy, depending on the guarantees of the removal check).

My exploration of new base learners instantiation will start with something like this.

\begin{algorithm}[H]
\caption{Generate Replay Subset}
\begin{algorithmic}[1]
\Require{Replay memory $\mathcal{D}$, Reward threshold $r_{\text{top}}$, Error threshold $e_{\text{top}}$, Random sample ratio $\rho_{\text{random}}$}
\State Initialize empty replay subset $\mathcal{D}_{\text{subset}} \leftarrow \emptyset$
\State Sort transitions in $\mathcal{D}$ by reward in descending order
\State Select top $50\%$ transitions by reward:
\begin{align*}
    \mathcal{D}_{\text{reward}} &\gets \text{Top } \lfloor 0.5 \cdot |\mathcal{D}| \rfloor \text{ transitions by reward}
\end{align*}
\State Sort transitions in $\mathcal{D}$ by error in descending order
\State Select top $30\%$ transitions by error:
\begin{align*}
    \mathcal{D}_{\text{error}} &\gets \text{Top } \lfloor 0.3 \cdot |\mathcal{D}| \rfloor \text{ transitions by error}
\end{align*}
\State Randomly sample $20\%$ transitions from $\mathcal{D}$:
\begin{align*}
    \mathcal{D}_{\text{random}} &\gets \text{Randomly sample } \lfloor 0.2 \cdot |\mathcal{D}| \rfloor \text{ transitions}
\end{align*}
\State Combine subsets:
\begin{align*}
    \mathcal{D}_{\text{subset}} &\gets \mathcal{D}_{\text{reward}} \cup \mathcal{D}_{\text{error}} \cup \mathcal{D}_{\text{random}}
\end{align*}
\State \Return $\mathcal{D}_{\text{subset}}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Instantiate New Base Learner}
\begin{algorithmic}[1]
\Require{Replay buffer $\mathcal{D}$, Ensemble $\mathcal{E}$, Mutation noise $\sigma$, Diversity threshold $\theta_D$}
\State Select a random base learner $\text{base} \in \mathcal{E}$
\State Generate mutation noise $\bm{\delta} \sim \mathcal{N}(0, \sigma^2)$
\State Instantiate new base learner:
\begin{align*}
    \bm{\theta}_{\text{new}} &\gets \bm{\theta}_{\text{base}} + \epsilon \cdot \bm{\delta} \\
    \bm{\phi}_{\text{new}} &\gets \bm{\phi}_{\text{base}} + \epsilon \cdot \bm{\delta}
\end{align*}
\State Call \textbf{Generate Replay Subset} to create training data:
\begin{align*}
    \text{replay subset} &\gets \textbf{Generate Replay Subset}(\mathcal{D}, r_{\text{top}}, e_{\text{top}}, \rho_{\text{random}})
\end{align*}
\State Train the new base learner on the replay subset
\State Perform diversity check:
\begin{align*}
    \text{diversity}_{\text{new}} &\gets \frac{1}{|\mathcal{S}| \cdot (|\mathcal{E}|-1)} 
    \sum_{s \in \mathcal{S}} \sum_{\substack{j=1 \\ j \neq \text{new}}}^{|\mathcal{E}|} 
    \left| \bm{a}_{\text{new}}(s) - \bm{a}_j(s) \right|_2
\end{align*}
\If{$\text{diversity}_{\text{new}} < \theta_D$}
    \State Discard new base learner
\Else
    \State Add new base learner to ensemble $\mathcal{E}$
\EndIf
\end{algorithmic}
\end{algorithm}

\subsection{Basic algorithm}

The starting point for the algorithm can be put together as follows:

\begin{algorithm}[H]
\caption{Dynamic Sunrise Algorithm}
\begin{algorithmic}[1]
\Require{Replay buffer size $N$, Minibatch size $K$, discount factor $\gamma$, learning rates $\lambda$, $\lambda_{\pi}$ and $\lambda_{Q}$, update frequency $C$, target smoothing coefficient $\tau$, Updates to Data ratio UTD, environemt steps $S$, neural network function approximator $Q$, neural network function approximator $\pi$ and initial entropy coefficient $\alpha$, ensemble size E, Bellman weight temperature $T$, }
\State Initialize replay memory $\mathcal{D} \leftarrow \emptyset$ with capacity $N$
\State Initialize policy $\pi_{\phi}$ with random weights $\phi$
\State Initialize both action-value function $Q_{\theta_i}$ with random weights 
$\theta_i$
\State Initialize $\bar{\theta}_{i} \leftarrow \theta_{i}$ for $i \in \{1,2\}$
\Repeat
    \For{ $S$ steps}
        \State // UCB exploration alternatively randomly select a base learner to be used for each episode.
        \State Collect $E$ action samples $\bm{a}_{t}^{(i)} = \pi_{\phi}(s_{t})$ for $i \in \{1,2,\ldots,E\}$
        \State select action: $a_{t} \sim \pi_{\phi}(a_{t} | s_{t})$
        \State Choose the action that maximizes the UCB exploration term:
        \State $a_{t} = \arg\max_{a} \left( Q_{\theta_1}(s_{t}, a) + \alpha \cdot \sigma(Q_{\theta_2}(s_{t}, a)) \right)$
        \State Observe reward $r_{t}$ and next state $s_{t+1}$
        \State Sample bootstrap masks $m_{t} = \{m_{t,i} \sim \text{Bernoulli}(0.5) \text{ for } i \in \{1,..., E\}\}$
        \State Store transition $(s_{t}, a_{t}, r_{t}, s_{t+1}, m_t)$ in $\mathcal{D}$
    \EndFor
    \If{Size of $\mathcal{D}$ is less than 10,000}
        \State Continue to next iteration without training of removal checks
    \EndIf
    \For{UTD times}
        \State Sample a batch $B$ of size $K$ from $\mathcal{D}$
        \For{each agent $i$}
            \State Use Mask $m_{t,i}$ to select transitions from $B$ for agent $i$
            \State Update Q-functions to minimize $(\text{sigmoid}(-\bar{Q}_{\text{std}}(s_{t+1}, a_{t+1})*T)+0.5)(Q_{\theta_i}(s_t, a_t)-r_t-\gamma \bar{V}(s_{t+1}))$
            \State Update policy to minimize $\mathbb{E}_{a_t \sim \pi_\phi}\left[ \alpha \log \pi_\phi (a_t|s_t) - Q_\theta(s_t, a_t)\right]$
            \State Update entropy coefficient: $\alpha \gets \alpha - \lambda \hat{\nabla}_{\alpha} J(\alpha)$
            \State Update target Q-networks: $\bar{\theta_{i}} \gets \tau\theta_{i} + (1 - \tau) \bar{\theta_{i}}$ for $i \in \{1,2\}$
        \EndFor
    \EndFor
    \State Perform removal check on the ensemble of base learners
    \For {Removed base learners}
        \State Instantiate new base learner using the instantiation algorithm
        \State Update $T$ to reflect the new ensemble size
    \EndFor
\Until{stopping criteria is met}

\end{algorithmic}
\end{algorithm}

Note that this algorithm does not take into account the various extra bits of information that will need to be stored for the proposed removal check and the instantiation of new base learners.

\section{Conclusion}

The sunrise algorithm is a simple but extensible framework for ensemble learnings. By making the ensemble dynamic it allows the algorithm to explore with greater speed and still return a single base learner.

The dynamic nature does not interfere with the ability of the original algorithm to be parallelized to speed up wall clock time. Furthermore it gives the ability for the ensemble to adapt to a changing environment by slowly pruning the less effective base learners and replacing them with new ones.

\end{document}