\chapter{Algorithms}\label{C:algorithms}

In the real world we don't get access to the transition function of the MDP. The agents only method to learn is through interacting with the environment. From this interaction we get a sample of the environment and must learn from it. Therefore stochastic learning methods must be used. Two learning ideas are core for many reinforcement learning algorithms. These are Monte Carlo \cite{suttonReinforcementLearningSecond2018} and Temporal Difference learning \cite{suttonTemporalCreditAssignment1984} \cite{suttonLearningPredictMethods1988}.

\section{Traditional algorithms}

\subsection{Value Based Methods}

As mentioned earlier value based method focus on learning a value function and from that function we can derive a policy. The action value function is more powerful as simply knowing the value of a state is not enough to make a decision. The action value function is defined as the expected return given a state and action. Thus all you need to do is take the action that gives you the maximum $Q$ value.


Monte Carlo learning works by estimating the value function as being the average return from that state.  A basic algorithm would look like this.

\begin{algorithm}
\caption{Monte Carlo Control with Exploring Starts}
\begin{algorithmic}[1]
\State Arbitrarily initialize $\pi(s) \in \mathcal{A}(s)$ and $Q(s,a) \in \mathbb{R}$
\State $\text{Returns}(S,A) \gets$ empty list
\For{each episode}
    \State Arbitrarily choose $S_0 \in \mathcal{S}, A_0 \in \mathcal{A}$
    \State Generate an episode by following $\pi$
    \State $G \gets 0$
    \For{each step of the episode $t = T-1, T-2, \dots, 0$}
        \State $G \gets \gamma G + R_{t+1}$
        \If{$(S_t, A_t)$ does not appear earlier in the episode}
            \State Append $G$ to $\text{Returns}(S_t, A_t)$
            \State $Q(S_t, A_t) \gets \text{average}(\text{Returns}(S_t, A_t))$
            \State $\pi(S_t) \gets \arg\max_{a} Q(S_t, a)$
        \EndIf
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

Because we need a well defined return Monte Carlo methods only work on episodic tasks. It will only update the states that it actually visits. This gives us the option of just exploring the state space area we are interested in learning about.

Temporal difference learning works different incrementally updating the value of a state with the received reward and the value of the next state \ref{eq:TDupdate}. As the update is using its own estimate it is \textit{bootstrapping}. Because of bootstrapping it can work with both episodic and continuing environments.

\begin{equation}
V(S_{t})=R_{t+1}+\gamma V(S_{t+1})
\label{eq:TDupdate}
\end{equation}

Below is a basic implementation of TD learning for a continuing environment, however it is easy to change it to episdoic by changing the loop to be over episodes.

\begin{algorithm}
\caption{TD(0)}
\begin{algorithmic}[1]
\State Initialize $Q(s,a)$ arbitrarily and set $Q(\text{terminal-state}) = 0$
\State Initialize $S$
\While{not converged}
    \State Take action $A$, observe $R$, $S'$
    \State Choose $A' \in \mathcal{A}(S')$ using policy derived from $Q$
    \State $Q(S,A) \gets Q(S,A) + \alpha \left[ R + \gamma Q(S', A') - Q(S,A) \right]$
    \State $S \gets S'$; $A \gets A'$
\EndWhile
\end{algorithmic}
\end{algorithm}

This particular update is known as TD(0) because it only looks one step into the future. However it can be generalised to be TD(n). One can see that if we had TD($\infty$) we would be back at Monte Carlo if working in episodic tasks.

The two methods of TD(n) and Monte carlo can be to get the best of both worlds using TD($\lambda$) methods \cite{suttonLearningPredictMethods1988}.


\subsection{Policy Based Methods}

A different way of the approaching the problem is by learning the policy directly without a learning a concept of how good a state is. The most common policy based method is called policy gradient methods \cite{suttonPolicyGradientMethods1999}. To use policy gradient methods you need to parametize the policy $\pi$ in any way as long as $\pi(a|s, \theta)$ is differentiable with respect to $\theta$.

The first of these policy gradient methods was the Reinforce algorithm \cite{williamsSimpleStatisticalGradientfollowing1992}. It is a Monte Carlo policy gradient method. The updates are made by moving the policy in the direction of the gradient of the log probability of the action taken. This works because it is proportional to the return which is given to use by the policy gradient theorem \cite{suttonPolicyGradientMethods1999}.

\begin{algorithm}
\caption{REINFORCE: Monte-Carlo Policy-Gradient Control (episodic) for $\pi_*$}
\begin{algorithmic}[1]
\Require A differentiable policy parameterization $\pi(a|s, \theta)$
\Require Step size $\alpha > 0$
\State Initialize policy parameter $\theta \in \mathbb{R}^d$ (e.g., to 0)
\Loop
  \State Generate an episode $S_0, A_0, R_1, \ldots, S_{T-1}, A_{T-1}, R_T$, following $\pi(\cdot|\cdot, \theta)$
  \For{$t = 0, 1, \ldots, T-1$}
    \State $G \leftarrow \sum_{k=t+1}^{T} \gamma^{k-t-1} R_k$
    \State $\theta \leftarrow \theta + \alpha \gamma^t G \nabla \ln \pi(A_t|S_t, \theta)$
  \EndFor
\EndLoop
\end{algorithmic}
\end{algorithm}

\subsection{Challenges of traditional methods}

There are main two issues with the simple methods of TD and Monte Carlo mentioned above. This is scalability and exploration.

Firstly with scalability we are storing the action value of a particular state in a large lookup table. This will immediately become a problem when dealing with large state or action spaces. Most real world application have tremendously large state/action spaces. Furthermore it is easy to imagine how most of the states have overlapping information and are actually really quite similar. Therefore instead of learning $Q$ directly we can try and learn an approximator $\hat{Q}$.

Second problem is exploration. As we learn we are finding better and better actions. These better action are taken which will form our trajectory. What can happen here is that we miss large chunks of the state space. To solve this we can instead use a sub optimal policy that deliberately takes exploratory actions. $\epsilon$-greedy is a good example of this as it will take a random action with probability $\epsilon$ and an optimal action the rest of the time. Alternatively you can use a different exploration policy to interact with the environment while you are learning the optimal. This method of learning from experience using a different policy is called off-policy learning. The previous methods we looked at are all on-policy.

\section{Modern Deep learning algorithms}\label{sec:MDLA}

One of the problems with the above tabular methods is that the policy and value functions are stored explicitly in effectively a large table lookup. This means that for every state and or action there needs to be a new entry. This is problematic as the state and action space of the problems get very large. To resolve this problem we can use function approximators and the best ones we have are neural networks \cite{hornikMultilayerFeedforwardNetworks1989}. This allows for the combination of the deep learning ideas within the reinforcement learning framework to give rise to deep reinforcement learning.

\subsection{Deep Q Networks}
\label{subsec:DQN}

One can take the TD(0) learning algorithm and apply the notion of off-policy learning we get SARSAMAX otherwise known as Q-Learning \cite{watkinsLearningDelayedReward1989} \cite{watkinsQlearning1992} . It works almost the same except the action value function is updated using the best possible action taken at the next state.

\begin{equation}
Q(S_{t}, A_{t}) \leftarrow Q(S_{t}, A_{t}) + \alpha \left( R_{t+1}+\gamma \max_{a} Q(S_{t+1}, a) - Q(S_{t}, A_{t}) \right) 
\end{equation}

As with TD(0) this fails when faced with a sufficiently large state and/or action space. Therefore we apply a neural network as an approximator $\hat{Q}$ for $Q$. The algorithm we get is called Deep-Q-Learning \cite{mnihPlayingAtariDeep2013}\cite{mnihHumanlevelControlDeep2015}. It was applied very successfully to match and exceed human level performance on a large variety of Atari 2600 games (Space invaders, pong etc).

Here is the algorithm that Mnih et al used in their papers \cite{mnihPlayingAtariDeep2013} \cite{mnihHumanlevelControlDeep2015}, with some notational differences to make it more explicit.

\begin{algorithm}[H]
\caption{Deep Q-Learning (DQN)}
\begin{algorithmic}[1]
\Require{Number of episodes $M$, replay memory capacity $N$, minibatch size $K$, discount factor $\gamma$, learning rate $\alpha$, update frequency $C$, exploration probability $\epsilon$. A neural network function approximiator$Q$}
\State Initialize replay memory $\mathcal{D} \leftarrow \emptyset$ with capacity $N$
\State Initialize action-value function $Q_{\theta}$ with random weights $\theta$
\State Initialize target action-value function $\hat{Q_{\theta^{-}}}$ with weights $\theta^{-} \leftarrow 0$
\For{episode in range(1, $M$)}
    \State Initialize sequence $s_{1} \leftarrow \{ x_{1} \}$ and preprocessed sequence $\phi_{1} \leftarrow \phi(s_{1})$
    \For{each step $t$ in episode}
        \State With probability $\epsilon$, select a random action $a_{t}$
        \State Otherwise, select $a_{t} \leftarrow \arg\max\limits_{a} Q(\phi(s_{t}), a; \theta)$
        \State Observe reward $r_{t}$ and next state $x_{t+1}$
        \State Set $s_{t+1} \leftarrow (s_{t}, a_{t}, x_{t+1})$ and preprocess $\phi_{t+1} \leftarrow \phi(s_{t+1})$
        \State Store transition $(\phi_{t}, a_{t}, r_{t}, \phi_{t+1})$ in $\mathcal{D}$
        \State Sample random minibatch $B$ of size $K$ of transitions from $\mathcal{D}$
        \For{each transition $(\phi_{j}, a_{j}, r_{j}, \phi_{j+1})$ in $B$}
            \[
            y_j \leftarrow \begin{cases} 
            r_j, & \text{if episode terminates at step } j+1 \\
            r_{j} + \gamma \max\limits_{a'} \hat{Q}(\phi_{j+1}, a'; \theta^{-}), & \text{otherwise}
            \end{cases}
            \]
            \State Compute loss $L_{j} \leftarrow (y_{j} - Q(\phi_{j}, a_{j}; \theta))^{2}$
        \EndFor
        \State Perform gradient descent step on the average loss: $\theta \leftarrow \theta - \alpha \nabla_{\theta} \frac{1}{|B|}\sum_{j} L_{j}$ \Comment{Alternative gradient steps could be used here (e.g Adam was used in the paper)}
        
        \If{$t$ mod $C$ == 0}
            \State Update target network: $\theta^{-} \leftarrow \theta$
        \EndIf
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

There are more features of this algorithm that are different than traditional Q-learning. These are important to solve problems the arise when you add a neural network approximator into the learning process of Q-learning.

The first of these ideas is called experience replay \cite{10.5555/168871}, which is collecting your experience in a replay memory $D$. Then to learn you can loop through your experience and extract as much as you can from your previous experiences. The idea in Deep Q-learning is that at each time step rather than updating $\hat{Q}$ with your current experience you update it using a sampled transition from $D$. This helps with an important problem which is that stochastic gradient descent assumes independent and identically distributed data (**i.i.d**). As experience is collected each transition will be dependent on the previous transition and will affect the next transition thus the independent assumption is not held. Thus randomly sampling from $D$ will give you a independent data set. This also makes DQN a off policy learning method

The second assumption is that the training data is identically distributed. Gradient descent is working towards the target $\gamma$. The problem is that the target involves our current prediction, so as we learn a better prediction our target will move. This results in a chase which decreases learning efficiency. The solution to this as seen in the algorithm is fixing $\hat{Q}$ for a fixed number of steps $C$. It uses $\hat{Q}$ in the target $\gamma_{j}$ but then updates $Q$ every $C$ updates. This gives the network a reasonable opportunity to tend towards the target $\gamma_{j}$ while still using a recent estimate of the action value.

Lastly is the concept of pre-processing the state to speed up the learning process. One can decrease the computational complexity of the neural network by simplifying the state using a transformation that in theory doesn't lose the meaningful information. Minh et al used this in their 2013 paper to make the video input; smaller, conform to the ratio their model needed and making it gray scale. We can see that this pre-processing step is the only example of domain specific knowledge that would be needed for an implementation of the algorithm to a task.

\subsection{Soft Actor-Critic}\label{subsec:SAC}

The DQN method discussed above, along with TD and MC learning, are all value-based methods. That is, they learn to evaluate how good a current situation is (whether that be a state or state-action), and then a policy can be generated by maximizing over the value function. Another approach is to directly learn the policy function. A combination of these ideas leads to the actor-critic framework.

In an actor-critic framework, there is a policy that controls how the agent acts, $\pi$, as well as a value function that evaluates how good the action taken was. A good way to update the policy is based on whether the result was better or worse than what the critic expected. Using the TD error of the critic for this is called A2C \cite{mnihAsynchronousMethodsDeep2016}.

However, there are still challenges in expanding A2C to high-dimensional continuous control tasks. Many current methods are difficult to stabilize and require carefully tuned hyperparameters. To address this, several improvements can be introduced to the actor-critic framework, resulting in Soft Actor-Critic (SAC) \cite{haarnojaSoftActorCriticOffPolicy2018}.

First, similar to DQN \cite{mnihPlayingAtariDeep2013, mnihHumanlevelControlDeep2015}, making it off-policy allows for much better sampling efficiency as more information is extracted from experience. More importantly, SAC employs entropy maximization. Traditional RL agents aim to maximize the expected sum of rewards. However, the maximum entropy RL framework modifies this objective to maximize both the expected reward and the entropy of the policy, leading to the following objective function:

\begin{equation}
J(\pi) = \sum_{t=0}^{T} \mathbb{E}_{(s_{t}, a_{t}) \sim \rho_{\pi}} \left[ r(s_{t}, a_{t}) + \alpha \mathcal{H}(\pi(\cdot|s_{t})) \right] 
\end{equation}

This entropy term encourages exploration, with the temperature parameter $\alpha$ determining the strength of this encouragement. Initially a hyperparameter in \cite{haarnojaSoftActorCriticOffPolicy2018}, the SAC algorithm was later modified to learn and adjust $\alpha$ throughout training \cite{haarnojaSoftActorCriticAlgorithms2019}. This not only improves efficiency—since choosing an optimal temperature is task-dependent and non-trivial \cite{haarnojaSoftActorCriticAlgorithms2019}—but also allows the policy to explore uncertain regions while being more exploitative in familiar areas.

The final SAC algorithm \cite{haarnojaSoftActorCriticAlgorithms2019} modified for clarity is shown below:

\begin{algorithm}[H]
\caption{Soft Actor-Critic Algorithm}
\label{alg:SAC}
\begin{algorithmic}[1]
\Require{Replay buffer size $N$, Minibatch size $K$, discount factor $\gamma$, learning rates $\lambda$, $\lambda_{\pi}$ and $\lambda_{Q}$, update frequency $C$, target smoothing coefficient $\tau$, Updates to Data ratio UTD, environemt steps $S$, neural network function approximator $Q$}, neural network function approximator $\pi$ and initial entropy coefficient $\alpha$.
\State Initialize replay memory $\mathcal{D} \leftarrow \emptyset$ with capacity $N$
\State Initialize policy $\pi_{\phi}$ with random weights $\phi$
\State Initialize both action-value function $Q_{\theta_i}$ with random weights 
$\theta_i$
\State Initialize $\bar{\theta}_{i} \leftarrow \theta_{i}$ for $i \in \{1,2\}$
\Repeat
    \For{ $S$ steps}
        \State select action: $a_{t} \sim \pi_{\phi}(a_{t} | s_{t})$
        \State Observe reward $r_{t}$ and next state $s_{t+1}$
        \State Store transition $(s_{t}, a_{t}, r_{t}, s_{t+1})$ in $\mathcal{D}$
    \EndFor
    \For{UTD times}
        \State Update Q-functions: $\theta_{i} \gets \theta_{i} - \lambda_{Q} \hat{\nabla}_{\theta_{i}} J_{Q}(\theta_{i})$ for $i \in \{1,2\}$
        \State Update policy: $\phi \gets \phi - \lambda_{\pi} \hat{\nabla}_{\phi} J_{\pi}(\phi)$
        \State Update entropy coefficient: $\alpha \gets \alpha - \lambda \hat{\nabla}_{\alpha} J(\alpha)$
        \State Update target Q-networks: $\bar{\theta_{i}} \gets \tau\theta_{i} + (1 - \tau) \bar{\theta_{i}}$ for $i \in \{1,2\}$
    \EndFor
\Until{stopping criteria is met}

\end{algorithmic}
\end{algorithm}

One noteworthy difference from conventional actor-critic methods is that SAC uses two Q-functions. While not strictly necessary, this significantly speeds up training \cite{haarnojaSoftActorCriticAlgorithms2019}.

Between 2018 and 2019, when SAC was first introduced, it outperformed other popular methods such as TD3 \cite{fujimotoAddressingFunctionApproximation2018}, DDPG \cite{lillicrapContinuousControlDeep2019}, and PPO \cite{schulmanProximalPolicyOptimization2017} in continuous control benchmarks.

\subsection{Proximal Polixy Optimizatin}\label{subsec:PPO}

One of the instabilities in RL algorithms is that a change in the actions taken by a policy can have large consequences, both good and bad. A natural idea is to try and limit the changes that can happen to a policy at any step. You can do this by using a surrogate objective function that is limited to be at least somewhat similar to the current policy. This was first implemented using a trust region (i.e an area that we can safely move the policy and is guaranteed to improve) and resulted in the TRPO algorithm \cite{schulmanTrustRegionPolicy2017}. Yet the most popular method is that of PPO \cite{schulmanProximalPolicyOptimization2017}, as it is computationally simpler. The method used in PPO is to use a clipped objective which results in a loss function like so: 
\begin{equation}
L^{\text{CLIP}}(s,a,\theta_k,\theta) = \min\left(
\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}  A^{\pi_{\theta_k}}(s,a), \;\;
\text{clip}\left(\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}, 1 - \epsilon, 1+\epsilon \right) A^{\pi_{\theta_k}}(s,a)
\right)
\end{equation}
Where the advantage function ($A^{\pi_{\theta_{k}}}(s,a)$) is a way of measuring how much better the situation is than expected. The ratio $\frac{\pi_{\theta}(a|s)}{\pi_{\theta_{k}}(a|s)}$ is used to measure the difference between the current policy and the old policy. This ratio is what is clipped and used as a scalar for the advantage function. As we see in the situation that the action was better ($A>0$) the clip function will apply and make sure that $L$ wont be too large, however it can be as small as 0. In the other case when things were worse than expected ($A < 0$) it capped to make $L$ not close to zero (note that $L$ will be negative), yet $L$ could be as large (in the negative sense) as it would like. The reasoning is that allowing for these large and negative $L$ means that we can do a better job of making sure that this action is not taken again.

The advantage function can be can be estimated in many ways . A simple advantage function could be actual return - estimated return ($G_{t} - V(s_{t})$) for episodic tasks, or TD error ($r_{t}+\gamma V(s_{t+1})-V(s_{t})$) for continuing tasks. The implementation in the paper use generalised advantage estimation which is an exponentially weighted sum of TD errors \cite{schulmanHighDimensionalContinuousControl2015}, as it better balances bias and variance. GAE is defined as:
\begin{equation}
\hat{A_{t}}^{GAE(\gamma, \lambda)} = \sum_{t=0}^{\infty}(\gamma\lambda)^{l}\delta_{t+l}^{V}
\end{equation}
Where $\delta_{t}^{V}=r_{t}+\gamma V(s_{t}+1)-V(s_{t})$. $\gamma$ is the usual discounting factor but this is expanded on with $\lambda$ which is from 0-1, at 0 it is TD(0) error and at 1 would be the full complete return.

With this advantage estimator we need to learn a value function. This puts PPO in the same style of actor-critic methods. It is however regarded as a Policy gradient method starting with the author Schulman designating it so \cite{schulmanProximalPolicyOptimization2017}.

The CLIP loss function can be augmented with other objectives to increase its effectiveness. For example using entropy maximisation to encourage exploring like what is done above with \ref{subsec:SAC}. This leads to this objective:

\begin{equation}
L^{\text{CLIP} + S}(s, a, \theta_{k, }\theta)=\hat{\mathbb{E}}_{t}\left[ L^{\text{CLIP}}(s,a, \theta_{k, }\theta)+cS\left[ \pi_{\theta}\right](s)  \right] 
\end{equation}

Where $S$ is the entropy of the policy. There is a large space of exploring different surrogate objective functions which incorporate the CLIP function.

Below is the algorithm from the paper \cite{schulmanProximalPolicyOptimization2017} with some modifications to make it clearer.:

\begin{algorithm}[H]
\caption{PPO Algorithm, Actor Critic style}
\begin{algorithmic}[1]
\Require{Clipping amount $\epsilon$, Minibatch size $K$, epoch $M$, discount factor $\gamma$, learning rates $\alpha_{\theta}$ and $\alpha_{\phi}$, number of actors $N$, environemt steps $T$, neural network function approximator $V$, neural network function approximator $\pi$, value function coefficient $c_{1}$ and entropy coefficient $c_{2}$}
\State Initialize policy $\pi_{\theta}$ and value function $V_{\phi}$ with random weights $\theta$ and $\phi$
\State $\theta_{old} \leftarrow \theta$
\Repeat
    \For{actor = 1, 2, \ldots, N \textbf{ in parallel}}
        \State Run policy $\pi_{\theta_{old}}$ in environment for $T$ timesteps
        \State Collect trajectory $(s_t, a_t, r_t, s_{t+1})_{t=0}^{T-1}$
        \State Compute advantage estimates $\hat{A}_1, \ldots, \hat{A}_T$ using GAE
        \State Compute returns $\hat{R}_t \leftarrow \sum_{i=t}^{T-1} \gamma^{i-t} r_i$
    \EndFor
    
    \State Combine data from all actors: $\mathcal{D} = \{(s_t^n, a_t^n, \hat{A}_t^n, \hat{R}_t^n)\}$ for $n=1,\ldots,N$ and $t=1,\ldots,T$
    \For{epoch = 1, 2, \ldots, M}
        \State Shuffle data $\mathcal{D}$ and split into minibatches of size $K \leq NT$
        \For{each minibatch $\mathcal{B} \subset \mathcal{D}$}
            \State For each $(s, a, \hat{A}, \hat{R}) \in \mathcal{B}$:
            \State \quad Compute ratio $r(\theta) = \frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)}$
            \State \quad Compute clipped surrogate objective:
            \State \quad $L_{clip}(\theta) = \min(r(\theta)\hat{A}, \text{clip}(r(\theta), 1-\epsilon, 1+\epsilon)\hat{A})$
            \State \quad Compute value function loss:
            \State \quad $L_{VF}(\phi) = (V_{\phi}(s) - \hat{R})^2$
            \State \quad Compute entropy bonus:
            \State \quad $S[\pi_{\theta}](s) = -\sum_a \pi_{\theta}(a|s) \log \pi_{\theta}(a|s)$
            
            \State \quad Total objective: $L(\theta, \phi) = \mathbb{E}[L_{clip}(\theta) - c_1 L_{VF}(\phi) + c_2 S[\pi_{\theta}](s)]$
            
            \State \quad Update $\theta$ using SGD or Adam optimizer: $\theta \leftarrow \theta + \alpha_{\theta} \nabla_{\theta} L(\theta, \phi)$
            \State \quad Update $\phi$ using SGD or Adam optimizer: $\phi \leftarrow \phi + \alpha_{\phi} \nabla_{\phi} L(\theta, \phi)$
        \EndFor
    \EndFor
    \State $\theta_{old} \leftarrow \theta$
\Until{ stopping criteria is met}
\end{algorithmic}
\end{algorithm}

\subsection{Twin Delayed Deep Deterministic Policy Gradient}\label{subsec:TD3}


A problem with using value function based methods with approximation is that they can lead to overestimation bias (an agent thinking a situation is better than it really is). This has been shown to be a problem in the contiunous control setting \cite{fujimotoAddressingFunctionApproximation2018}. A way to resolve this overestimation error is to use two critics to estimate the value of the action taken and then use the minimum of the two to update the policy. This is called Twin Delayed Deep Deterministic Policy Gradient \cite{fujimotoAddressingFunctionApproximation2018} and is a variant of DDPG \cite{lillicrapContinuousControlDeep2016} that builds on Double Q-Learning \cite{hasseltDeepReinforcementLearning2015}. It is a actor critic method that also uses experience replay, target networks and policy noise to stabilise learning.

The policy noise method is not seen in the other algorithms mentioned above. The clipped policy noise in the critic target is used to make sure that the critics aren't overfitting to peaks. This works on the intuition that similiar actions should have similar values.

The algorithm from the original paper \cite{fujimotoAddressingFunctionApproximation2018} with slight modifications for clarity:

\begin{algorithm}[H]
\caption{Twin Delayed Deep Deterministic Policy Gradient (TD3)}
\begin{algorithmic}[1]
\Require{Replay buffer size $N$, Minibatch size $K$, discount factor $\gamma$, learning rates $\alpha_{\pi}$ and $\alpha_{Q}$, target smoothing coefficient $\tau$, policy noise $\sigma$, target policy noise clip $c$, target policy noise $\tilde{\sigma}$, update frequency $d$ and neural network function approximator $Q$ and $\pi$}
\State Initialize critic networks $Q_{\theta_1}$, $Q_{\theta_2}$, and actor network $\pi_{\phi}$ with random parameters $\theta_1$, $\theta_2$, $\phi$
\State Initialize target networks $\theta'_1 \leftarrow \theta_1$, $\theta'_2 \leftarrow \theta_2$, $\phi' \leftarrow \phi$
\State Initialize replay buffer $\mathcal{D}$
\Repeat
    \State increment timestep $t$
    \State Select action with exploration noise $a \sim \pi_{\phi}(s) + \epsilon$, $\epsilon \sim \mathcal{N}(0, \sigma)$ and observe reward $r$ and new state $s'$
    \State Store transition tuple $(s, a, r, s')$ in $\mathcal{D}$
    \State Sample mini-batch of $K$ transitions $(s, a, r, s')$ from $\mathcal{D}$
    \State Initialize critic loss $L_{\theta_1} \leftarrow 0$, $L_{\theta_2} \leftarrow 0$
    \For{$i = 1$ to $K$}
        \State Add noise to target action: $\tilde{a}_i \leftarrow \pi_{\phi'}(s'_i) + \text{clip}(\epsilon_i, -c, c)$ where $\epsilon_i \sim \mathcal{N}(0, \tilde{\sigma})$
        \State Compute target Q-value: $y_i \leftarrow r_i + \gamma \min(Q_{\theta'_1}(s'_i, \tilde{a}_i), Q_{\theta'_2}(s'_i, \tilde{a}_i))$
        \State Accumulate critic losses: $L_{\theta_j} \leftarrow L_{\theta_j} + (y_i - Q_{\theta_j}(s_i, a_i))^2$ for $j = 1, 2$
    \EndFor
    \State Update critics: $\theta_i \leftarrow \theta_i - \alpha_Q \nabla_{\theta_i}(L_{\theta_i}/K)$ for $i = 1, 2$

    \If{$t \bmod d$}
        \State Update $\phi$ by the deterministic policy gradient:
        \State $\nabla_{\phi}J(\phi) = K^{-1} \sum_{i}^{K} \nabla_a Q_{\theta_1}(s_i, a)|_{a=\pi_{\phi}(s_i)} \nabla_{\phi}\pi_{\phi}(s_i)$
        \State Update target networks:
        \State $\theta'_i \leftarrow \tau\theta_i + (1 - \tau)\theta'_i$
        \State $\phi' \leftarrow \tau\phi + (1 - \tau)\phi'$
    \EndIf
\Until{stopping criteria is met}
\end{algorithmic}
\end{algorithm}


\section{State of the Art Algorithms}

Since the introduction of the modern deep reinforcement learning algorithms, there have been many new algorithms that have been developed. These generally build on the ideas from the first generation of deep reinforcement learning algorithms. I will go into detail about 4 algorithms which are actor critic methods and build on the ideas of SAC \ref{subsec:SAC}. These are:

\begin{description}
    \item{Truncated Quantile Critics (TQC) \cite{kuznetsovControllingOverestimationBias2020} which uses an ensemble of critics and a truncated distributional critic with the goal of reducing the overestimation bias.}
    \item{Randomized Ensemble Double Q-Learning (REDQ) \cite{chenRandomizedEnsembledDouble2021} which uses an ensemble of critics which are trained towards a target made up of the minimum of a random subset of the critics. This allows for a more stable learning process that can handle Updates to Data ratio (UTD) which are greater than 1.}
    \item{Dropout Q-functions (DROQ) \cite{hiraokaDropoutQFunctionsDoubly2022} which is a variant of REDQ that uses smaller number of critics and instead adds layer dropout and layer normalization to handle a UTD greater than 1.}
    \item{Cross Q-learning (CrossQ) \cite{bhattCrossQBatchNormalization2024} which builds upon SAC to increase sample efficiency yet keep computation similar to SAC. It does this by removing the target networks, making the critic function take both current and the next state action pair to predict both the current and next step action value in one forward pass. applying batch normalization.}

\end{description}

\subsection{Truncated Quantile Critics}\label{subsec:TQC}

One fo the challenges in learning an accurate critic function is the overestimations bias. This comes from the fact that the critic is updated towards the maximum of the Q-values. TQC \cite{kuznetsovControllingOverestimationBias2020} mitigates this by using multiple distributional critics and truncating the top quantile of the distributions.

The distributional reinforcement learning framework \cite{bellemareDistributionalPerspectiveReinforcement2017} is different from traditional Q-learning because rather than learning the expected value for the state action pair, it learns the return.

$$Z_{\psi_n}(s, a) := \frac{1}{M} \sum_{m=1}^{M} \delta \left(\theta_{\psi_n}^m(s, a) \right)$$

It approximates the distribution with $M$ number of quantiles. each predicting the value of the return at a given quantile $\theta_{\psi}^{m}$. 

Rather than just having a single critic, TQC uses $N$ critics. It takes all of the quantiles from the critics and mixes them together to form a single distribution. From there it truncates the top $k$ values. Kuznetsov et al \cite{kuznetsovControllingOverestimationBias2020} posits that the order of mixing then truncating rather than truncating and mixing is important. These quantiles are then used to create a target which the critics are trained towards. In practice target critic networks are also used as well to stabilize the learning process.

The algorithm from the paper \cite{kuznetsovControllingOverestimationBias2020} with slight modifications is shown below:

\begin{algorithm}[H]
\caption{Truncated Quantile Critics (TQC)}
\begin{algorithmic}[1]
\Require{Replay buffer size $N$, Minibatch size $K$, discount factor $\gamma$, learning rates; $\lambda_{\alpha}, \lambda_{\pi}, \lambda_{Q}$, number of critics $C$, truncation amount $k$, number of quantiles $M$, target critic update rate $\beta$, Updates to Data ratio UTD, and neural network function approximator $Z$ and $\pi$}
\State Initialize replay buffer $\mathcal{D}$
\State Initialize policy $\pi_{\phi}$ and $C$ critics $Z_{\psi_i}$, $Z_{\overline{\psi}_i}$ with random parameters $\psi_i$
\State Set $\alpha = 1$ and $\mathcal{H}_T = -\dim \mathcal{A}$

\Repeat
    \State Run policy $\pi_{\phi}$ in environment until termination or truncation
    \State Append trajectories $(s_t, a_t, r_t, s_{t+1})_{t=0}^{T-1}$ to $\mathcal{D}$
    \For{UTD times}
        \State Sample a batch $B$ of size $K$ from the replay $\mathcal{D}$
        \State Update $\alpha$ with gradient descent using
        \[
        \nabla_{\alpha} \frac{1}{K} \sum_{(s,a)\in B} \log \alpha (-\log \pi_{\phi}(a|s)- \mathcal{H}_T) 
        \]
        \State $\phi$ with gradient descent using
        \[
        \nabla_{\phi} \frac{1}{K} \sum_{(s,a) \in B}\left(\alpha \log \pi_\phi(a|s) - \frac{1}{NM} \sum_{n=1}^{N} \sum_{m=1}^{M} \theta_{\psi_n}^m(s, a)\right)
        \]
        \State Update $\psi_n$ with gradient descent using for $n=1,\ldots,C$:
        \[
            \nabla_{\psi_i} \frac{1}{KkCM} \sum_{(s,a) \in B} \sum_{m=1}^M \sum_{i=1}^{kC} \rho_{\tau_m}^H \left( y_i(s,a) - \theta_{\psi_n}^{m}(s,a)\right)
        \]
        \vspace{1pt}
        \State  $\overline{\psi}_n \leftarrow  \beta \psi_n  + (1 - \beta) \overline{\psi}_n $ for $n\in[1..C]$
    \EndFor
\Until{stopping criteria is met}
\end{algorithmic}
\end{algorithm}

We can see that this algorithm is similar to that of SAC \ref{subsec:SAC} in that it uses a replay buffer, target networks and entropy maximization. It varies from SAC in that the interaction of the environments is done until termination or truncation where the truncation. That means that a complete episode is completed in-between each update. It is also important to note that the policy is optimized using the untruncated distributions from the critics because there is not the same feedback that causes an overestimation bias.

\subsection{Randomized Ensemble Double Q-Learning}\label{subsec:REDQ}

Chen et al \cite{chenRandomizedEnsembledDouble2021} made an algorithm to increase its updates to data ratio (UTD) to be greater than 1. Naively increasing the UTD to be grater than 1 in SAC \ref{subsec:SAC} has poor results, which comes from the instability in the learning. To stabilize the learning REDQ using an ensemble of critics with a target made up of the minimum of a random subset of the critics.

The ensemble of $C$ critics is trained towards a target which is the minimum of a random subset $M$ of the critics. This target is in turn used to update all of the critics. The policy is optimized using the average of the critics. With an $C = 10$ and $M = 2$ the algorithm was able to handle a UTD of 20 \cite{chenRandomizedEnsembledDouble2021}. The authors reference implementation of the algorithm is built of SAC \ref{subsec:SAC} and interesting turns back into SAC if $C=M=2$ and $UTD=1$.

At the time it was the first model-free algorithm to be able to handle a UTD greater than 1. This higher ratio allows for an increase in sample efficiency as it is extracting more information from the replay buffer per time step. This does however increase the computational cost as each time step requires $C \times UTD$ gradient descent steps and $M$ forward passes. In practice this means that it is doing 200 gradient descents per time step rather than the 1 that TQC \ref{subsec:TQC} and SAC \ref{subsec:SAC} do.

Pseudo code for the algorithm taken from the paper \cite{chenRandomizedEnsembledDouble2021} with some modifications for clarity is shown below:


\begin{algorithm}[H]
\caption{Randomized Ensembled Double Q-learning (REDQ)}
\begin{algorithmic}[1]
\Require{Replay buffer size $N$, Minibatch size $K$, discount factor $\gamma$, learning rates $\lambda_{\pi}, \lambda_{Q}$, number of critics $C$, target critic update rate $\beta$, entropy coefficient $\alpha$ critic subsets $M$, Updates to Data ratio UTD, and neural network function approximator $Q$ and $\pi$}
\State Initialize policy parameters $\theta$, $C$ Q-function parameters $\phi_{i}$, $i=1,\ldots,N$, empty replay buffer $\mathcal{D}$. Set target parameters $\phi_{\text {targ}, i} \leftarrow \phi_{i}, \text{ for } i =1, 2, \ldots, N$

\Repeat
    \State Select action: $a_{t} \sim \pi_{\theta}(a_{t} | s_{t})$ and observe reward $r_{t}$ and next state $s_{t+1}$
    \State Store transition $(s_{t}, a_{t}, r_{t}, s_{t+1})$ in $\mathcal{D}$
    \For {$UTD$ times}
        \State Sample a batch $B$ of size $K$ from the replay $\mathcal{D}$
        \State Sample a set $\cal{M}$ of $M$ distinct indices from $\{1, 2, \ldots, C\}$
        \State Compute the Q target $y$ (same for all of the $C$ Q-functions):
        \[
        y=r+\gamma\left(\min _{i\in \cal{M}} Q_{\phi_{\text {targ},i}}\left(s^{\prime}, \tilde{a}^{\prime}\right)-\alpha \log \pi_{\theta}\left(\tilde{a}^{\prime} \mid s^{\prime}\right)\right),\quad \tilde{a}^{\prime} \sim \pi_{\theta}\left(\cdot \mid s^{\prime}\right)
        \]
        \For{$i=1,\ldots,C$}
            \State Update $\phi_i$ with gradient descent using
            \[
            \nabla_{\phi} \frac{1}{K} \sum_{\left(s, a, r, s^{\prime} \right) \in B}\left(Q_{\phi_{i}}(s, a)-y\right)^{2} 
            \]
            \State Update target networks with $\phi_{\operatorname{targ},i} \leftarrow \beta \phi_{\operatorname{targ},i}+(1-\beta) \phi_{i}$
        \EndFor
    \EndFor
    \State Update policy parameters $\theta$ with gradient ascent using
    \[
    \nabla_{\theta} \frac{1}{K} \sum_{s \in B}\left(\frac{1}{C}\sum_{i =1}^{C} Q_{\phi_{i}}\left(s, \tilde{a}_{\theta}(s)\right)-\alpha \log \pi_{\theta}\left(\tilde{a}_{\theta}(s) | s\right)\right),
    \quad \tilde{a}_{\theta}(s) \sim \pi_{\theta}(\cdot \mid s)
    \]
\Until{stopping criteria is met}
\end{algorithmic}
\end{algorithm}

The algorithm uses some of the same stabilization tricks like replay buffer, entropy maximization and target networks. However the structure is a bit different, it only does one action followed by some number of gradient updates and lastly a single policy update. 


\subsection{Dropout Q-functions}\label{subsec:DROQ}

The big draw back of REDQ \ref{subsec:REDQ} is that it requires a large number of critics to be able to handle a UTD greater than 1. Hiraoka et al \cite{hiraokaDropoutQFunctionsDoubly2022} proposed a method called Dropout Q-functions (DroQ) which uses a smaller number of critics and instead uses dropout and layer normalization to handle the UTD. This allows for a similar sample efficiency at about half the computational cost (DroQ is about 2x faster than REDQ) \cite{hiraokaDropoutQFunctionsDoubly2022}.

Dropout layers \cite{srivastavaDropoutSimpleWay2014} are used within the critic networks to randomly drop out neurons during training. It is used within the deep learning community to help prevent overfitting. The intuition being that randomly removing neurons forces the networks to learn features that are not reliant on single neurons. However in this setting it is being used to model the uncertainty in the Q-value. Layer normalization \cite{baLayerNormalization2016} is used after the dropout layer which increases the effectiveness of the dropout layer.

As DroQ uses a smaller ensemble of critics it does to take a subset and instead uses all of the critics to create the critic target for critic updating. This is in contrast to REDQ \ref{subsec:REDQ} which uses a random subset of the critics. Other than this the algorithm is almost identical to REDQ as seen below (red highlights the differences):

\begin{algorithm}[H]
\caption{Dropout Q-functions (DroQ)}
\begin{algorithmic}[1]
\Require{Replay buffer size $N$, Minibatch size $K$, discount factor $\gamma$, learning rates $\lambda_{\pi}, \lambda_{Q}$, number of critics $C$, target critic update rate $\beta$, entropy coefficient $\alpha$, \textcolor{red}{dropout rate}, Updates to Data ratio UTD, and neural network function approximator $Q$ and $\pi$}
\State Initialize policy parameters $\theta$, $C$ Q-function parameters $\phi_{i}$, $i=1,\ldots,N$, empty replay buffer $\mathcal{D}$. Set target parameters $\phi_{\text {targ}, i} \leftarrow \phi_{i}, \text{ for } i =1, 2, \ldots, N$

\Repeat
    \State Select action: $a_{t} \sim \pi_{\theta}(a_{t} | s_{t})$ and observe reward $r_{t}$ and next state $s_{t+1}$
    \State Store transition $(s_{t}, a_{t}, r_{t}, s_{t+1})$ in $\mathcal{D}$
    \For {$UTD$ times}
        \State Sample a mini-batch $B=\left\{\left(s, a, r, s^{\prime} \right)\right\}$ from $\mathcal{D}$ of size $K$
        \State Compute the Q target $y$ (same for all of the $C$ Q-functions):
        \[
        y=r+\gamma\left(\textcolor{red}{\min _{i\in C} Q_{Dr, \phi_{\text {targ},i}}}\left(s^{\prime}, \tilde{a}^{\prime}\right)-\alpha \log \pi_{\theta}\left(\tilde{a}^{\prime} \mid s^{\prime}\right)\right),\quad \tilde{a}^{\prime} \sim \pi_{\theta}\left(\cdot \mid s^{\prime}\right)
        \]
        \For{$i=1,\ldots,C$}
            \State Update $\phi_i$ with gradient descent using
            \[
            \nabla_{\phi} \frac{1}{K} \sum_{\left(s, a \right) \in B}\left(\textcolor{red}{Q_{Dr, \phi_{i}}}(s, a)-y\right)^{2} 
            \]
            \State Update target networks with $\phi_{\operatorname{targ},i} \leftarrow \beta \phi_{\operatorname{targ},i}+(1-\beta) \phi_{i}$
        \EndFor
    \EndFor
    \State Update policy parameters $\theta$ with gradient ascent using
    \[
    \nabla_{\theta} \frac{1}{K} \sum_{s \in B}\left(\frac{1}{C}\sum_{i =1}^{C} \textcolor{red}{Q_{Dr,\phi_{i}}}\left(s, \tilde{a}_{\theta}(s)\right)-\alpha \log \pi_{\theta}\left(\tilde{a}_{\theta}(s) | s\right)\right),
    \quad \tilde{a}_{\theta}(s) \sim \pi_{\theta}(\cdot \mid s)
    \]
\Until{stopping criteria is met}
\end{algorithmic}
\end{algorithm}


\subsection{Cross Q-learning}\label{subsec:CROSSQ}


The goal of both REDQ \ref{subsec:REDQ} and DROQ \ref{subsec:DROQ} is to increase the sample efficiency and they achieve it by using a UTD $\gg$ 1 (typically around 20). However they both come at a computational cost which increases which increases linearly with the UTD. CrossQ \cite{bhattCrossQBatchNormalization2024} instead is just a simple improvement on SAC \ref{subsec:SAC} which removes the target networks allowing it to use batch normalization \cite{ioffeBatchRenormalizationReducing2017}. This combination allows it to achieve higher sample efficiency than REDQ \ref{subsec:REDQ} and DROQ \ref{subsec:DROQ} while being computationally simpler resulting in about 4 times faster (using wall clock measurements). It gains this computational advantage by using a UTD of 1. Much wider critic networks (~2000) are also used as it further increases the sample efficiency \cite{bhattCrossQBatchNormalization2024}.

To allow the batch normalization to work without the target networks we combine the calculation of the current Q-value and the next Q-value. This is done by concatenating the current state and action with the next state and action. This means that the loss function is changed from what we see in SAC \ref{subsec:SAC} which is:

\begin{equation}
    J_Q(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ \left( Q_{\theta}(s,a) - r - \gamma Q_{\bar{\theta}}(s',\pi_{\phi}(s')) \right)^2 \right]
\end{equation}

\noindent To no longer use the target network and instead the output from a single Q-function:

\begin{equation}
    J_{Q_{BRN}}(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ \left( q_t - r - \gamma q_{t+1}) \right)^2 \right] \text{ and } Q_{BRN,\theta}( \left[ \begin{matrix} S \\ S' \end{matrix}\right], \left[ \begin{matrix} A \\ \pi_\psi(s') \end{matrix}\right]) = \left[ \begin{matrix} q_t \\ q_{t+1} \end{matrix}\right]
\end{equation}

This concatenation is important as it allows the batch normalization to work as half of the data is from the distribution made with the current policy and the other half is from the distribution made with the updated policy. Because it is half and half none of the data is out of distribution so that normlization works as expected.

These changes are applied to SAC \ref{subsec:SAC} in the paper however the changes could be applied to other off policy TD learning based algorithms. The algorithm is almost identical to the SAC algorithm which can be found here \ref{alg:SAC}. The differences are highlighted in red below:


\begin{algorithm}[H]
\caption{Cross Q-learning (CrossQ)}
\label{alg:CROSSQ}
\begin{algorithmic}[1]
\Require{Replay buffer size $N$, Minibatch size $K$, discount factor $\gamma$, learning rates $\lambda$, $\lambda_{\pi}$ and $\lambda_{Q}$, update frequency $C$, target smoothing coefficient $\tau$, Updates to Data ratio UTD, environemt steps $S$, neural network function approximator $Q$}, neural network function approximator $\pi$ and initial entropy coefficient $\alpha$.
\State Initialize replay memory $\mathcal{D} \leftarrow \emptyset$ with capacity $N$
\State Initialize policy $\pi_{\phi}$ with random weights $\phi$
\State Initialize both action-value function $Q_{\theta_i}$ with random weights 
$\theta_i$
\Repeat
    \For{ $S$ steps}
        \State select action: $a_{t} \sim \pi_{\phi}(a_{t} | s_{t})$
        \State Observe reward $r_{t}$ and next state $s_{t+1}$
        \State Store transition $(s_{t}, a_{t}, r_{t}, s_{t+1})$ in $\mathcal{D}$
    \EndFor
    \For{UTD times}
        \State Update Q-functions: $\theta_{i} \gets \theta_{i} - \textcolor{red}{\lambda_{Q_{BRN}}} \hat{\nabla}_{\theta_{i}} \textcolor{red}{J_{Q_{BRN}}}(\theta_{i})$ for $i \in \{1,2\}$
        \State Update policy: $\phi \gets \phi - \lambda_{\pi} \hat{\nabla}_{\phi} J_{\pi}(\phi)$
        \State Update entropy coefficient: $\alpha \gets \alpha - \lambda \hat{\nabla}_{\alpha} J(\alpha)$
    \EndFor
\Until{stopping criteria is met}

\end{algorithmic}
\end{algorithm}
The differences are in the loss function as mentioned above, critic networks that have batch normalisation and the removal of target networks.

\subsection{SUNRISE}\label{subsec:SUNRISE}

Sunrise stands for 'Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning' \cite{leeSUNRISESimpleUnified2021}. It is built with the goal of using the ensemble to help stabilize learning and manage the exploration and exploitation trade off. The ideas of Sunrise can be applied to any off policy RL algorithm, however for illustration purposes focus will be put on the SAc variant that uses Soft actor-critic agents \cite{haarnojaSoftActorCriticOffPolicy2018}. The algorithm works by applying two different methods. Firstly is weighted bellman backups which works with the idea that the amount to update a network should be determined by how confident the agent is in in the given update. An understanding of confident can be acieved by using the variance of the ensemble of Q-functions. The actual weight is achieved using the equation.

\begin{equation}
	w(s,a)=\sigma (-\bar{Q}_{\text{std}}(s,a) * T) + 0.5
\end{equation}

\noindent Where $\sigma$ is the sigmoid function and $T$ is the temperature. One can observe that it will always be in the bounds of [0.5, 1] which means implementing the weighted bellman backup is as simple as multipling the loss function by the weight function.

The second method that Sunrise uses is UCB exploration which involves choosing the next action from the ensemble so that it maximises both the expected reward and the uncertainty. This is to directly help manage the exploration and exploitation tradeoff. The uncertainty is modeled with $\gamma Q_{\text{std}}(s, a)$ where $\gamma$ is a hyperparameter which determines the importance of the uncertainty in choosing the next action. Importantly it can noted that as the sunrise algorithm learns the std of the Q-functions will decrease as they converge on $Q^*$.

As this is an ensemble method it relies on the fact that each of the base learners are different from one another. In sunrise variation is guaranteed using random initialisation and different learning experience. The random initilisation is simply using a differnet initilised weights for the actor and critic networks. Different learning experience is achieved by giving each transition a mask which determines which base learners will use it when they update paramters. This mask is made using the Bernoulli distribution.

Put togather the algorithm for Sunrise is as follows.

\begin{algorithm}[H]
\caption{Sunrise Algorithm}
\begin{algorithmic}[1]
\Require{Replay buffer size $N$, Minibatch size $K$, discount factor $\gamma$, learning rates $\lambda$, $\lambda_{\pi}$ and $\lambda_{Q}$, update frequency $C$, target smoothing coefficient $\tau$, Updates to Data ratio UTD, environment steps $S$, neural network function approximator $Q$, neural network function approximator $\pi$ and initial entropy coefficient $\alpha$, ensemble size $E$, Bellman weight temperature $T$}
\State Initialize replay memory $\mathcal{D} \leftarrow \emptyset$ with capacity $N$
\State Initialize policy $\pi_{\phi}$ with random weights $\phi$
\State Initialize both action-value function $Q_{\theta_i}$ with random weights 
$\theta_i$
\State Initialize $\bar{\theta}_{i} \leftarrow \theta_{i}$ for $i \in \{1,2\}$
\Repeat
    \For{ $S$ steps}
        \State // UCB exploration alternatively randomly select a base learner to be used for each episode.
        \State Collect $E$ action samples $\bm{a}_{t}^{(i)} = \pi_{\phi}(s_{t})$ for $i \in \{1,2,\ldots,E\}$
        \State select action: $a_{t} \sim \pi_{\phi}(a_{t} | s_{t})$
        \State Choose the action that maximizes the UCB exploration term:
        \State $a_{t} = \arg\max_{a} \left( Q_{\theta_1}(s_{t}, a) + \alpha \cdot \sigma(Q_{\theta_2}(s_{t}, a)) \right)$
        \State Observe reward $r_{t}$ and next state $s_{t+1}$
        \State Sample bootstrap masks $m_{t} = \{m_{t,i} \sim \text{Bernoulli}(0.5) \text{ for } i \in \{1,..., E\}\}$
        \State Store transition $(s_{t}, a_{t}, r_{t}, s_{t+1}, m_t)$ in $\mathcal{D}$
    \EndFor
    \If{Size of $\mathcal{D}$ is less than 10,000}
        \State Continue to next iteration without training of removal checks
    \EndIf
    \For{UTD times}
        \State Sample a batch $B$ of size $K$ from $\mathcal{D}$
        \For{each agent $i$}
            \State Use Mask $m_{t,i}$ to select transitions from $B$ for agent $i$
            \State Update Q-functions to minimize $(\text{sigmoid}(-\bar{Q}_{\text{std}}(s_{t+1}, a_{t+1})*T)+0.5)(Q_{\theta_i}(s_t, a_t)-r_t-\gamma \bar{V}(s_{t+1}))$
            \State Update policy to minimize $\mathbb{E}_{a_t \sim \pi_\phi}\left[ \alpha \log \pi_\phi (a_t|s_t) - Q_\theta(s_t, a_t)\right]$
            \State Update entropy coefficient: $\alpha \gets \alpha - \lambda \hat{\nabla}_{\alpha} J(\alpha)$
            \State Update target Q-networks: $\bar{\theta_{i}} \gets \tau\theta_{i} + (1 - \tau) \bar{\theta_{i}}$ for $i \in \{1,2\}$
        \EndFor
    \EndFor
\Until{stopping criteria is met}

\end{algorithmic}
\end{algorithm}
