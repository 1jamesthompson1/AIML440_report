\chapter{Conclusions}\label{C:con}

Reinforcement Learning is a powerful framework for learning how to act. The recent combination of addition of deep learning has made reinforcement learning more powerful than ever. In this report I have covered the basics of reinforcement learning and the algorithms that are used to solve the problem. I have also evaluated some foundational algorithms and shown that they are capable of solving complex problems. 

There are many ways of splitting the field of reinforcement learning. The binary splits we have looked at are model based and model free, value based and policy based and on policy and off policy. However the different solutions can merge the boundary between these splits. For example actor critic methods are explicitly both value based and policy based. Below is a table which helps summarise the differences between the modern algorithms.

\begin{table}[h]
    \footnotesize
    \centering
    \renewcommand{\arraystretch}{1.4} % Increases row spacing for readability
    \begin{tabularx}{\textwidth}{p{1.3cm} X X X X}
        \hline
        \textbf{Feature} & \textbf{DQN} & \textbf{SAC} & \textbf{PPO} & \textbf{TD3} \\
        \hline
        \textbf{Algorithm type}       & Value based      & Actor-critic         & Policy based (with actor critic style)              & Actor-critic  \\
        \textbf{Policy learning}      & Off policy        & Off policy           & On policy            & Off policy    \\
        \textbf{Learnt policy}        & Deterministic     & Stochastic           & Stochastic           & Deterministic \\
        \textbf{Exploration strategy} & $\epsilon$-greedy & entropy maximisation & entropy maximisation & noise         \\
        \textbf{Action space}         & Discrete          & Continuous           & Both                 & Continuous    \\
        \textbf{Extra features} & Experience replay, ensemble Q-functions & Experience replay, ensemble Q-functions & - & Experience replay \\
        \hline
    \end{tabularx}
    \caption{Differences between modern deep reinforcement learning algorithms.}
\end{table}

These models at their time of release were all considered state of the are but due to the fast moving field are now transitioning to more foundational model which the latest algorithms are building on. There is unknown potential in the applicability of current algorithms as well as unknown power in new algorithms. Some of the current limitations of the algorithms are the sample efficiency and the stability of the algorithms. Furthermore there is the larger problem of generalisation, which is getting an agent to act intelligently on varied environments that it has never seen before with minimal to no new training.

The field of reinforcement learning is still in its infancy and there is much to be discovered. The combination of deep learning and reinforcement learning has opened up many possibilities and the future is bright and exciting.