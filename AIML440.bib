@thesis{10.5555/168871,
  type = {phdthesis},
  title = {Reinforcement Learning for Robots Using Neural Networks},
  author = {Lin, Long-Ji},
  date = {1992},
  institution = {Carnegie Mellon University},
  location = {USA},
  abstract = {Reinforcement learning agents are adaptive, reactive, and self-supervised. The aim of this dissertation is to extend the state of the art of reinforcement learning and enable its applications to complex robot-learning problems. In particular, it focuses on two issues. First, learning from sparse and delayed reinforcement signals is hard and in general a slow process. Techniques for reducing learning time must be devised. Second, most existing reinforcement learning methods assume that the world is a Markov decision process. This assumption is too strong for many robot tasks of interest. This dissertation demonstrates how we can possibly overcome the slow learning problem and tackle non-Markovian environments, making reinforcement learning more practical for realistic robot tasks: (1) Reinforcement learning can be naturally integrated with artificial neural networks to obtain high-quality generalization, resulting in a significant learning speedup. Neural networks are used in this dissertation, and they generalize effectively even in the presence of noise and a large of binary and real-valued inputs. (2) Reinforcement learning agents can save many learning trials by using an action model, which can be learned on-line. With a model, an agent can mentally experience the effects of its actions without actually executing them. Experience replay is a simple technique that implements this idea, and is shown to be effective in reducing the number of action executions required. (3) Reinforcement learning agents can take advantage of instructive training instances provided by human teachers, resulting in a significant learning speedup. Teaching can also help learning agents avoid local optima during the search for optimal control. Simulation experiments indicate that even a small amount of teaching can save agents many learning trials. (4) Reinforcement learning agents can significantly reduce learning time by hierarchical learning–they first solve elementary learning problems and then combine solutions to the elementary problems to solve a complex problem. Simulation experiments indicate that a robot with hierarchical learning can solve a complex problem, which otherwise is hardly solvable within a reasonable time. (5) Reinforcement learning agents can deal with a wide range of non-Markovian environments by having a memory of their past. Three memory architectures are discussed. They work reasonably well for a variety of simple problems. One of them is also successfully applied to a nontrivial non-Markovian robot task.The results of this dissertation rely on computer simulation, including (1) an agent operating in a dynamic and hostile environment and (2) a mobile robot operating in a noisy and non-Markovian environment. The robot simulator is physically realistic. This dissertation concludes that it is possible to build artificial agents than can acquire complex control policies effectively by reinforcement learning.}
}

@article{bellemareArcadeLearningEnvironment2013,
  title = {The {{Arcade Learning Environment}}: {{An Evaluation Platform}} for {{General Agents}}},
  shorttitle = {The {{Arcade Learning Environment}}},
  author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  date = {2013-06-14},
  journaltitle = {Journal of Artificial Intelligence Research},
  shortjournal = {jair},
  volume = {47},
  eprint = {1207.4708},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {253--279},
  issn = {1076-9757},
  doi = {10.1613/jair.3912},
  url = {http://arxiv.org/abs/1207.4708},
  urldate = {2025-03-05},
  abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/4FRLGSC8/Bellemare et al. - 2013 - The Arcade Learning Environment An Evaluation Platform for General Agents.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/P38JCCAF/1207.html}
}

@online{bowlingSettlingRewardHypothesis2023,
  title = {Settling the {{Reward Hypothesis}}},
  author = {Bowling, Michael and Martin, John D. and Abel, David and Dabney, Will},
  date = {2023-09-16},
  eprint = {2212.10420},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.10420},
  url = {http://arxiv.org/abs/2212.10420},
  urldate = {2025-03-04},
  abstract = {The reward hypothesis posits that, “all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).” We aim to fully settle this hypothesis. This will not conclude with a simple affirmation or refutation, but rather specify completely the implicit requirements on goals and purposes under which the hypothesis holds.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/E3TK3CYU/Bowling et al. - 2023 - Settling the Reward Hypothesis.pdf}
}

@online{chenRandomizedEnsembledDouble2021,
  title = {Randomized {{Ensembled Double Q-Learning}}: {{Learning Fast Without}} a {{Model}}},
  shorttitle = {Randomized {{Ensembled Double Q-Learning}}},
  author = {Chen, Xinyue and Wang, Che and Zhou, Zijian and Ross, Keith},
  date = {2021-03-18},
  eprint = {2101.05982},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2101.05982},
  url = {http://arxiv.org/abs/2101.05982},
  urldate = {2025-03-13},
  abstract = {Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio {$>>$} 1; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio {$>>$} 1.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/L7GLA5GZ/Chen et al. - 2021 - Randomized Ensembled Double Q-Learning Learning Fast Without a Model.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/RRUZ3VZL/2101.html}
}

@incollection{dingAlgorithmCheatsheet2020,
  title = {Algorithm {{Cheatsheet}}},
  booktitle = {Deep {{Reinforcement Learning}}: {{Fundamentals}}, {{Research}} and {{Applications}}},
  author = {Ding, Zihan},
  editor = {Dong, Hao and Ding, Zihan and Zhang, Shanghang},
  date = {2020},
  pages = {489--514},
  publisher = {Springer},
  location = {Singapore},
  doi = {10.1007/978-981-15-4095-0_20},
  url = {https://doi.org/10.1007/978-981-15-4095-0_20},
  urldate = {2025-03-10},
  abstract = {In this chapter, we summarized the algorithms introduced throughout the book, which are categorized into four sections of deep learning, reinforcement learning, deep reinforcement learning, and advanced deep reinforcement learning. The pseudo-code is provided for each algorithm to facilitate the learning process of readers.},
  isbn = {978-981-15-4095-0},
  langid = {english},
  keywords = {Algorithm,Deep learning,Pseudo-code,Reinforcement learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/8R5WD7A8/Ding - 2020 - Algorithm Cheatsheet.pdf}
}

@book{dongDeepReinforcementLearning2020,
  title = {Deep {{Reinforcement Learning}}: {{Fundamentals}}, {{Research}} and {{Applications}}},
  shorttitle = {Deep {{Reinforcement Learning}}},
  editor = {Dong, Hao and Ding, Zihan and Zhang, Shanghang},
  date = {2020},
  publisher = {Springer},
  location = {Singapore},
  doi = {10.1007/978-981-15-4095-0},
  url = {http://link.springer.com/10.1007/978-981-15-4095-0},
  urldate = {2025-03-10},
  isbn = {978-981-15-4094-3 978-981-15-4095-0},
  langid = {english},
  keywords = {Deep Learning,Deep reinforcement learning,DRL,Machine Learning,Reinforcement Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/VUG7E759/Dong et al. - 2020 - Deep Reinforcement Learning Fundamentals, Research and Applications.pdf}
}

@online{fujimotoAddressingFunctionApproximation2018,
  title = {Addressing {{Function Approximation Error}} in {{Actor-Critic Methods}}},
  author = {Fujimoto, Scott and family=Hoof, given=Herke, prefix=van, useprefix=false and Meger, David},
  date = {2018-10-22},
  eprint = {1802.09477},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1802.09477},
  url = {http://arxiv.org/abs/1802.09477},
  urldate = {2025-03-11},
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/P2JPUPLJ/Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-Critic Methods.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/55W9DCJ2/1802.html}
}

@online{haarnojaSoftActorCriticAlgorithms2019,
  title = {Soft {{Actor-Critic Algorithms}} and {{Applications}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  date = {2019-01-29},
  eprint = {1812.05905},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1812.05905},
  url = {http://arxiv.org/abs/1812.05905},
  urldate = {2025-03-09},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy; that is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as challenging real-world tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/PHTLMRVV/Haarnoja et al. - 2019 - Soft Actor-Critic Algorithms and Applications.pdf}
}

@online{haarnojaSoftActorCriticOffPolicy2018,
  title = {Soft {{Actor-Critic}}: {{Off-Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  shorttitle = {Soft {{Actor-Critic}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  date = {2018-08-08},
  eprint = {1801.01290},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1801.01290},
  url = {http://arxiv.org/abs/1801.01290},
  urldate = {2025-03-10},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/JDDFUMTD/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/QSUYUIWA/1801.html}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  date = {1989-01-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90020-8},
  url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
  urldate = {2025-03-06},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/DLM2NLFJ/Hornik et al. - 1989 - Multilayer feedforward networks are universal approximators.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/5SUJRYSL/0893608089900208.html}
}

@online{lillicrapContinuousControlDeep2019,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  date = {2019-07-05},
  eprint = {1509.02971},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1509.02971},
  url = {http://arxiv.org/abs/1509.02971},
  urldate = {2025-03-11},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/RNBIXCFV/Lillicrap et al. - 2019 - Continuous control with deep reinforcement learning.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/3WG56EK4/1509.html}
}

@article{mccarthyWHATARTIFICIALINTELLIGENCE2007,
  title = {{{WHAT IS ARTIFICIAL INTELLIGENCE}}?},
  author = {McCarthy, John},
  date = {2007-11-12},
  abstract = {This article for the layman answers basic questions about artificial intelligence. The opinions expressed here are not all consensus opinion among researchers in AI.},
  langid = {english},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/XQSTB5WU/McCarthy - WHAT IS ARTIFICIAL INTELLIGENCE.pdf}
}

@online{mnihAsynchronousMethodsDeep2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  date = {2016-06-16},
  eprint = {1602.01783},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1602.01783},
  url = {http://arxiv.org/abs/1602.01783},
  urldate = {2025-03-12},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/VQCQQ3W8/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/93X5S7IX/1602.html}
}

@article{mnihHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  date = {2015-02},
  journaltitle = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  url = {https://www.nature.com/articles/nature14236},
  urldate = {2025-03-05},
  abstract = {An artificial agent is developed that learns to play~a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a~performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  langid = {english},
  keywords = {Computer science},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/LJVE5JTV/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf}
}

@online{mnihPlayingAtariDeep2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  date = {2013-12-19},
  eprint = {1312.5602},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1312.5602},
  url = {http://arxiv.org/abs/1312.5602},
  urldate = {2025-03-02},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/DBSUNWF2/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/XNSUC4AR/1312.html}
}

@online{ProximalPolicyOptimization,
  title = {Proximal {{Policy Optimization}} — {{Spinning Up}} Documentation},
  url = {https://spinningup.openai.com/en/latest/algorithms/ppo.html#},
  urldate = {2025-03-12},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/AXGZ2GSL/ppo.html}
}

@online{roderickImplementingDeepQNetwork2017,
  title = {Implementing the {{Deep Q-Network}}},
  author = {Roderick, Melrose and MacGlashan, James and Tellex, Stefanie},
  date = {2017-11-20},
  eprint = {1711.07478},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1711.07478},
  url = {http://arxiv.org/abs/1711.07478},
  urldate = {2025-03-05},
  abstract = {The Deep Q-Network proposed by Mnih et al. [2015] has become a benchmark and building point for much deep reinforcement learning research. However, replicating results for complex systems is often challenging since original scientific publications are not always able to describe in detail every important parameter setting and software engineering solution. In this paper, we present results from our work reproducing the results of the DQN paper. We highlight key areas in the implementation that were not covered in great detail in the original paper to make it easier for researchers to replicate these results, including termination conditions and gradient descent algorithms. Finally, we discuss methods for improving the computational performance and provide our own implementation that is designed to work with a range of domains, and not just the original Arcade Learning Environment [Bellemare et al., 2013].},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/78TIBVK3/Roderick et al. - 2017 - Implementing the Deep Q-Network.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/M3H8SNSY/1711.html}
}

@online{schulmanHighDimensionalContinuousControl2015,
  title = {High-{{Dimensional Continuous Control Using Generalized Advantage Estimation}}},
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  date = {2015-06-08},
  eprint = {1506.02438},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1506.02438},
  url = {http://arxiv.org/abs/1506.02438},
  urldate = {2025-03-12},
  abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Computer Science - Systems and Control},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/9UQGZHYL/Schulman et al. - 2018 - High-Dimensional Continuous Control Using Generalized Advantage Estimation.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/B39U64B6/1506.html}
}

@online{schulmanProximalPolicyOptimization2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  date = {2017-08-28},
  eprint = {1707.06347},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1707.06347},
  url = {http://arxiv.org/abs/1707.06347},
  urldate = {2025-03-11},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/SKFAPYF3/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/X5VYUDJG/1707.html}
}

@online{schulmanProximalPolicyOptimization2017a,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  date = {2017-08-28},
  eprint = {1707.06347},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1707.06347},
  url = {http://arxiv.org/abs/1707.06347},
  urldate = {2025-03-11},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/8D4CZ3M6/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/MJQG7TCK/1707.html}
}

@online{schulmanTrustRegionPolicy2017,
  title = {Trust {{Region Policy Optimization}}},
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  date = {2017-04-20},
  eprint = {1502.05477},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1502.05477},
  url = {http://arxiv.org/abs/1502.05477},
  urldate = {2025-03-12},
  abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/EYQPF5PH/Schulman et al. - 2017 - Trust Region Policy Optimization.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/7L7GWTPW/1502.html}
}

@article{silverRewardEnough2021,
  title = {Reward Is Enough},
  author = {Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard S.},
  date = {2021-10-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {299},
  pages = {103535},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2021.103535},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370221000862},
  urldate = {2025-03-04},
  abstract = {In this article we hypothesise that intelligence, and its associated abilities, can be understood as subserving the maximisation of reward. Accordingly, reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence, including knowledge, learning, perception, social intelligence, language, generalisation and imitation. This is in contrast to the view that specialised problem formulations are needed for each ability, based on other signals or objectives. Furthermore, we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities, and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence.},
  keywords = {Artificial general intelligence,Artificial intelligence,Reinforcement learning,Reward},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/6I8KY6BU/S0004370221000862.html}
}

@article{suttonLearningPredictMethods1988,
  title = {Learning to Predict by the Methods of Temporal Differences},
  author = {Sutton, Richard S.},
  date = {1988-08-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {3},
  number = {1},
  pages = {9--44},
  issn = {1573-0565},
  doi = {10.1007/BF00115009},
  url = {https://doi.org/10.1007/BF00115009},
  urldate = {2025-03-05},
  abstract = {This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
  langid = {english},
  keywords = {Artificial Intelligence,connectionism,credit assignment,evaluation functions,Incremental learning,prediction},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/E3BTFSG8/Sutton - 1988 - Learning to predict by the methods of temporal differences.pdf}
}

@book{suttonReinforcementLearningSecond2018,
  title = {Reinforcement {{Learning}}, Second Edition: {{An Introduction}}},
  shorttitle = {Reinforcement {{Learning}}, Second Edition},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {2018-11-13},
  edition = {2nd edition},
  publisher = {Bradford Books},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
  isbn = {978-0-262-03924-6},
  langid = {english},
  pagetotal = {552},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/CN8F6KMW/Sutton and Barto - 2018 - Reinforcement Learning, second edition An Introduction.pdf}
}

@online{suttonRewardHypothesis2004,
  title = {The Reward Hypothesis},
  author = {Sutton, Richard},
  date = {2004-09-12},
  url = {http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html},
  urldate = {2025-03-04},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/RFR3XQDU/rewardhypothesis.html}
}

@thesis{suttonTemporalCreditAssignment1984,
  title = {Temporal Credit Assignment in Reinforcement Learning},
  author = {Sutton, Richard Stuart},
  date = {1984},
  institution = {University of Massachusetts Amherst},
  url = {https://search.proquest.com/openview/16b17efcf37774c8c0a5e29706dc8098/1?pq-origsite=gscholar&cbl=18750&diss=y},
  urldate = {2025-03-12}
}

@thesis{watkinsLearningDelayedReward1989,
  title = {Learning from {{Delayed}} Reward},
  author = {Watkins, Chris},
  date = {1989-05},
  institution = {King's College London},
  location = {England},
  url = {https://www.cs.rhul.ac.uk/~chrisw/thesis.html},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/K6JU4HFV/Watkins - 1989 - Learning from Delayed reward.pdf}
}

@article{watkinsQlearning1992,
  title = {Q-Learning},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  date = {1992-05-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {8},
  number = {3},
  pages = {279--292},
  issn = {1573-0565},
  doi = {10.1007/BF00992698},
  url = {https://doi.org/10.1007/BF00992698},
  urldate = {2025-03-05},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
  langid = {english},
  keywords = {Artificial Intelligence,asynchronous dynamic programming,Q-learning,reinforcement learning,temporal differences},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/ZQ3XHW35/Watkins et Dayan - 1992 - Q-learning.pdf}
}
