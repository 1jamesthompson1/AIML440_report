@phdthesis{10.5555/168871,
  title = {Reinforcement Learning for Robots Using Neural Networks},
  author = {Lin, Long-Ji},
  year = {1992},
  address = {USA},
  abstract = {Reinforcement learning agents are adaptive, reactive, and self-supervised. The aim of this dissertation is to extend the state of the art of reinforcement learning and enable its applications to complex robot-learning problems. In particular, it focuses on two issues. First, learning from sparse and delayed reinforcement signals is hard and in general a slow process. Techniques for reducing learning time must be devised. Second, most existing reinforcement learning methods assume that the world is a Markov decision process. This assumption is too strong for many robot tasks of interest. This dissertation demonstrates how we can possibly overcome the slow learning problem and tackle non-Markovian environments, making reinforcement learning more practical for realistic robot tasks: (1) Reinforcement learning can be naturally integrated with artificial neural networks to obtain high-quality generalization, resulting in a significant learning speedup. Neural networks are used in this dissertation, and they generalize effectively even in the presence of noise and a large of binary and real-valued inputs. (2) Reinforcement learning agents can save many learning trials by using an action model, which can be learned on-line. With a model, an agent can mentally experience the effects of its actions without actually executing them. Experience replay is a simple technique that implements this idea, and is shown to be effective in reducing the number of action executions required. (3) Reinforcement learning agents can take advantage of instructive training instances provided by human teachers, resulting in a significant learning speedup. Teaching can also help learning agents avoid local optima during the search for optimal control. Simulation experiments indicate that even a small amount of teaching can save agents many learning trials. (4) Reinforcement learning agents can significantly reduce learning time by hierarchical learning--they first solve elementary learning problems and then combine solutions to the elementary problems to solve a complex problem. Simulation experiments indicate that a robot with hierarchical learning can solve a complex problem, which otherwise is hardly solvable within a reasonable time. (5) Reinforcement learning agents can deal with a wide range of non-Markovian environments by having a memory of their past. Three memory architectures are discussed. They work reasonably well for a variety of simple problems. One of them is also successfully applied to a nontrivial non-Markovian robot task.The results of this dissertation rely on computer simulation, including (1) an agent operating in a dynamic and hostile environment and (2) a mobile robot operating in a noisy and non-Markovian environment. The robot simulator is physically realistic. This dissertation concludes that it is possible to build artificial agents than can acquire complex control policies effectively by reinforcement learning.},
  school = {Carnegie Mellon University}
}

@misc{baLayerNormalization2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = {2016},
  month = jul,
  number = {arXiv:1607.06450},
  eprint = {1607.06450},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1607.06450},
  urldate = {2025-04-03},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/EVGCGAU5/Ba et al. - 2016 - Layer Normalization.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/3JCM7PCC/1607.html}
}

@article{bellemareArcadeLearningEnvironment2013,
  title = {The {{Arcade Learning Environment}}: {{An Evaluation Platform}} for {{General Agents}}},
  shorttitle = {The {{Arcade Learning Environment}}},
  author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  year = {2013},
  month = jun,
  journal = {Journal of Artificial Intelligence Research},
  volume = {47},
  eprint = {1207.4708},
  primaryclass = {cs},
  pages = {253--279},
  issn = {1076-9757},
  doi = {10.1613/jair.3912},
  urldate = {2025-03-05},
  abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/4FRLGSC8/Bellemare et al. - 2013 - The Arcade Learning Environment An Evaluation Platform for General Agents.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/P38JCCAF/1207.html}
}

@misc{bellemareDistributionalPerspectiveReinforcement2017,
  title = {A {{Distributional Perspective}} on {{Reinforcement Learning}}},
  author = {Bellemare, Marc G. and Dabney, Will and Munos, R{\'e}mi},
  year = {2017},
  month = jul,
  number = {arXiv:1707.06887},
  eprint = {1707.06887},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1707.06887},
  urldate = {2025-04-02},
  abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/9DQ5YYSK/Bellemare et al. - 2017 - A Distributional Perspective on Reinforcement Learning.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/JANGQACT/1707.html}
}

@book{bellmanDynamicProgramming1957,
  title = {Dynamic {{Programming}}},
  author = {Bellman, Richard},
  year = {1957},
  publisher = {Princeton University Press},
  googlebooks = {rZW4ugAACAAJ},
  langid = {english},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/JJDZ3HFN/Bellman - 1957 - Dynamic Programming.pdf}
}

@misc{bhattCrossQBatchNormalization2024,
  title = {{{CrossQ}}: {{Batch Normalization}} in {{Deep Reinforcement Learning}} for {{Greater Sample Efficiency}} and {{Simplicity}}},
  shorttitle = {{{CrossQ}}},
  author = {Bhatt, Aditya and Palenicek, Daniel and Belousov, Boris and Argus, Max and Amiranashvili, Artemij and Brox, Thomas and Peters, Jan},
  year = {2024},
  month = mar,
  number = {arXiv:1902.05605},
  eprint = {1902.05605},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1902.05605},
  urldate = {2025-04-02},
  abstract = {Sample efficiency is a crucial problem in deep reinforcement learning. Recent algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the critic per environment sample. However, this comes at the expense of a greatly increased computational cost. To reduce this computational burden, we introduce CrossQ: A lightweight algorithm for continuous control tasks that makes careful use of Batch Normalization and removes target networks to surpass the current state-of-the-art in sample efficiency while maintaining a low UTD ratio of 1. Notably, CrossQ does not rely on advanced bias-reduction schemes used in current methods. CrossQ's contributions are threefold: (1) it matches or surpasses current state-of-the-art methods in terms of sample efficiency, (2) it substantially reduces the computational cost compared to REDQ and DroQ, (3) it is easy to implement, requiring just a few lines of code on top of SAC.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/LB3AWAZG/Bhatt et al. - 2024 - CrossQ Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplic.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/9W74UP8Q/1902.html}
}

@misc{bowlingSettlingRewardHypothesis2023,
  title = {Settling the {{Reward Hypothesis}}},
  author = {Bowling, Michael and Martin, John D. and Abel, David and Dabney, Will},
  year = {2023},
  month = sep,
  number = {arXiv:2212.10420},
  eprint = {2212.10420},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.10420},
  urldate = {2025-03-04},
  abstract = {The reward hypothesis posits that, ``all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).'' We aim to fully settle this hypothesis. This will not conclude with a simple affirmation or refutation, but rather specify completely the implicit requirements on goals and purposes under which the hypothesis holds.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/E3TK3CYU/Bowling et al. - 2023 - Settling the Reward Hypothesis.pdf}
}

@misc{chenRandomizedEnsembledDouble2021,
  title = {Randomized {{Ensembled Double Q-Learning}}: {{Learning Fast Without}} a {{Model}}},
  shorttitle = {Randomized {{Ensembled Double Q-Learning}}},
  author = {Chen, Xinyue and Wang, Che and Zhou, Zijian and Ross, Keith},
  year = {2021},
  month = mar,
  number = {arXiv:2101.05982},
  eprint = {2101.05982},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2101.05982},
  urldate = {2025-03-13},
  abstract = {Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio {$>>$} 1; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio {$>>$} 1.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/L7GLA5GZ/Chen et al. - 2021 - Randomized Ensembled Double Q-Learning Learning Fast Without a Model.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/RRUZ3VZL/2101.html}
}

@incollection{dingAlgorithmCheatsheet2020,
  title = {Algorithm {{Cheatsheet}}},
  booktitle = {Deep {{Reinforcement Learning}}: {{Fundamentals}}, {{Research}} and {{Applications}}},
  author = {Ding, Zihan},
  editor = {Dong, Hao and Ding, Zihan and Zhang, Shanghang},
  year = {2020},
  pages = {489--514},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-15-4095-0_20},
  urldate = {2025-03-10},
  abstract = {In this chapter, we summarized the algorithms introduced throughout the book, which are categorized into four sections of deep learning, reinforcement learning, deep reinforcement learning, and advanced deep reinforcement learning. The pseudo-code is provided for each algorithm to facilitate the learning process of readers.},
  isbn = {978-981-15-4095-0},
  langid = {english},
  keywords = {Algorithm,Deep learning,Pseudo-code,Reinforcement learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/8R5WD7A8/Ding - 2020 - Algorithm Cheatsheet.pdf}
}

@book{dongDeepReinforcementLearning2020,
  title = {Deep {{Reinforcement Learning}}: {{Fundamentals}}, {{Research}} and {{Applications}}},
  shorttitle = {Deep {{Reinforcement Learning}}},
  editor = {Dong, Hao and Ding, Zihan and Zhang, Shanghang},
  year = {2020},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-15-4095-0},
  urldate = {2025-03-10},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-981-15-4094-3 978-981-15-4095-0},
  langid = {english},
  keywords = {Deep Learning,Deep reinforcement learning,DRL,Machine Learning,Reinforcement Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/VUG7E759/Dong et al. - 2020 - Deep Reinforcement Learning Fundamentals, Research and Applications.pdf}
}

@misc{fujimotoAddressingFunctionApproximation2018,
  title = {Addressing {{Function Approximation Error}} in {{Actor-Critic Methods}}},
  author = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  year = {2018},
  month = oct,
  number = {arXiv:1802.09477},
  eprint = {1802.09477},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.09477},
  urldate = {2025-03-11},
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/P2JPUPLJ/Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-Critic Methods.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/55W9DCJ2/1802.html}
}

@misc{haarnojaSoftActorCriticAlgorithms2019,
  title = {Soft {{Actor-Critic Algorithms}} and {{Applications}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  year = {2019},
  month = jan,
  number = {arXiv:1812.05905},
  eprint = {1812.05905},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.05905},
  urldate = {2025-03-09},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy; that is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as challenging real-world tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/PHTLMRVV/Haarnoja et al. - 2019 - Soft Actor-Critic Algorithms and Applications.pdf}
}

@misc{haarnojaSoftActorCriticOffPolicy2018,
  title = {Soft {{Actor-Critic}}: {{Off-Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  shorttitle = {Soft {{Actor-Critic}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  year = {2018},
  month = aug,
  number = {arXiv:1801.01290},
  eprint = {1801.01290},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1801.01290},
  urldate = {2025-03-10},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/JDDFUMTD/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/QSUYUIWA/1801.html}
}

@misc{hasseltDeepReinforcementLearning2015,
  title = {Deep {{Reinforcement Learning}} with {{Double Q-learning}}},
  author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
  year = {2015},
  month = dec,
  number = {arXiv:1509.06461},
  eprint = {1509.06461},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1509.06461},
  urldate = {2025-03-17},
  abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/YES3LB3H/Hasselt et al. - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/QHVV4LZX/1509.html}
}

@misc{hiraokaDropoutQFunctionsDoubly2022,
  title = {Dropout {{Q-Functions}} for {{Doubly Efficient Reinforcement Learning}}},
  author = {Hiraoka, Takuya and Imagawa, Takahisa and Hashimoto, Taisei and Onishi, Takashi and Tsuruoka, Yoshimasa},
  year = {2022},
  month = mar,
  number = {arXiv:2110.02034},
  eprint = {2110.02034},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.02034},
  urldate = {2025-04-02},
  abstract = {Randomized ensembled double Q-learning (REDQ) (Chen et al., 2021b) has recently achieved state-of-the-art sample efficiency on continuous-action reinforcement learning benchmarks. This superior sample efficiency is made possible by using a large Q-function ensemble. However, REDQ is much less computationally efficient than non-ensemble counterparts such as Soft Actor-Critic (SAC) (Haarnoja et al., 2018a). To make REDQ more computationally efficient, we propose a method of improving computational efficiency called DroQ, which is a variant of REDQ that uses a small ensemble of dropout Q-functions. Our dropout Q-functions are simple Q-functions equipped with dropout connection and layer normalization. Despite its simplicity of implementation, our experimental results indicate that DroQ is doubly (sample and computationally) efficient. It achieved comparable sample efficiency with REDQ, much better computational efficiency than REDQ, and comparable computational efficiency with that of SAC.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/W3F4VCBW/Hiraoka et al. - 2022 - Dropout Q-Functions for Doubly Efficient Reinforcement Learning.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/83YCGEL2/2110.html}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90020-8},
  urldate = {2025-03-06},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/DLM2NLFJ/Hornik et al. - 1989 - Multilayer feedforward networks are universal approximators.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/5SUJRYSL/0893608089900208.html}
}

@inproceedings{ioffeBatchRenormalizationReducing2017,
  title = {Batch {{Renormalization}}: {{Towards Reducing Minibatch Dependence}} in {{Batch-Normalized Models}}},
  shorttitle = {Batch {{Renormalization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ioffe, Sergey},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-04-03},
  abstract = {Batch Normalization is quite effective at accelerating and improving the training of deep models. However, its effectiveness diminishes when the training minibatches are small, or do not consist of independent samples. We hypothesize that this is due to the dependence of model layer inputs on all the examples in the minibatch, and different activations being produced between training and inference. We propose Batch Renormalization, a simple and effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire minibatch. Models trained with Batch Renormalization perform substantially better than batchnorm when training with small  or non-i.i.d. minibatches. At the same time, Batch Renormalization retains the benefits of batchnorm such as insensitivity to initialization and training efficiency.},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/V3I7YJWU/Ioffe - 2017 - Batch Renormalization Towards Reducing Minibatch Dependence in Batch-Normalized Models.pdf}
}

@misc{kuznetsovControllingOverestimationBias2020,
  title = {Controlling {{Overestimation Bias}} with {{Truncated Mixture}} of {{Continuous Distributional Quantile Critics}}},
  author = {Kuznetsov, Arsenii and Shvechikov, Pavel and Grishin, Alexander and Vetrov, Dmitry},
  year = {2020},
  month = may,
  number = {arXiv:2005.04269},
  eprint = {2005.04269},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.04269},
  urldate = {2025-03-31},
  abstract = {The overestimation bias is one of the major impediments to accurate off-policy learning. This paper investigates a novel way to alleviate the overestimation bias in a continuous control setting. Our method---Truncated Quantile Critics, TQC,---blends three ideas: distributional representation of a critic, truncation of critics prediction, and ensembling of multiple critics. Distributional representation and truncation allow for arbitrary granular overestimation control, while ensembling provides additional score improvements. TQC outperforms the current state of the art on all environments from the continuous control benchmark suite, demonstrating 25\% improvement on the most challenging Humanoid environment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/NI4E2BCJ/Kuznetsov et al. - 2020 - Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/Z2PIEF6H/2005.html}
}

@misc{leePokaxpokaSunrise2025,
  title = {Pokaxpoka/Sunrise},
  author = {Lee, Kimin},
  year = {2025},
  month = may,
  urldate = {2025-06-01},
  abstract = {SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning},
  keywords = {codebase,deep-learning,deep-neural-networks,deep-q-learning,deep-q-network,deep-reinforcement-learning,dm-control,model-free,mujoco,off-policy,rainbow,reinforcement-learning,rl,sac,soft-actor-critic}
}

@misc{leeSUNRISESimpleUnified2021,
  title = {{{SUNRISE}}: {{A Simple Unified Framework}} for {{Ensemble Learning}} in {{Deep Reinforcement Learning}}},
  shorttitle = {{{SUNRISE}}},
  author = {Lee, Kimin and Laskin, Michael and Srinivas, Aravind and Abbeel, Pieter},
  year = {2021},
  month = jun,
  number = {arXiv:2007.04938},
  eprint = {2007.04938},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2007.04938},
  urldate = {2025-06-01},
  abstract = {Off-policy deep reinforcement learning (RL) has been successful in a range of challenging domains. However, standard off-policy RL algorithms can suffer from several issues, such as instability in Q-learning and balancing exploration and exploitation. To mitigate these issues, we present SUNRISE, a simple unified ensemble method, which is compatible with various off-policy RL algorithms. SUNRISE integrates two key ingredients: (a) ensemble-based weighted Bellman backups, which re-weight target Q-values based on uncertainty estimates from a Q-ensemble, and (b) an inference method that selects actions using the highest upper-confidence bounds for efficient exploration. By enforcing the diversity between agents using Bootstrap with random initialization, we show that these different ideas are largely orthogonal and can be fruitfully integrated, together further improving the performance of existing off-policy RL algorithms, such as Soft Actor-Critic and Rainbow DQN, for both continuous and discrete control tasks on both low-dimensional and high-dimensional environments. Our training code is available at https://github.com/pokaxpoka/sunrise.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/R3NBG6MV/Lee et al. - 2021 - SUNRISE A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/BQCW3PU6/2007.html}
}

@misc{lillicrapContinuousControlDeep2016,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  year = {2016},
  month = feb,
  number = {arXiv:1509.02971},
  eprint = {1509.02971},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1509.02971},
  urldate = {2025-03-17},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/9ELZNBI8/Lillicrap et al. - 2016 - Continuous control with deep reinforcement learning.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/JIE4N2FR/1509.html}
}

@misc{lillicrapContinuousControlDeep2019,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  year = {2019},
  month = jul,
  number = {arXiv:1509.02971},
  eprint = {1509.02971},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1509.02971},
  urldate = {2025-03-11},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/RNBIXCFV/Lillicrap et al. - 2019 - Continuous control with deep reinforcement learning.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/3WG56EK4/1509.html}
}

@article{liuPolicyEnsembleGradient2023a,
  title = {Policy Ensemble Gradient for Continuous Control Problems in Deep Reinforcement Learning},
  author = {Liu, Guoqiang and Chen, Gang and Huang, Victoria},
  year = {2023},
  month = sep,
  journal = {Neurocomputing},
  volume = {548},
  pages = {126381},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2023.126381},
  urldate = {2025-03-30},
  abstract = {Policy gradient algorithms for reinforcement learning (RL) have successfully tackled a broad range of high-dimensional continuous RL problems, including many challenging robotic control problems. These algorithms can be largely divided into two categories, i.e., on-policy algorithms and off-policy algorithms. Off-policy deep RL (DRL) algorithms enjoy better sample efficiency than and often outperform on-policy algorithms. However, cutting-edge off-policy algorithms still suffer from the low-quality estimation of policy gradients, resulting in compromised learning performance and high sensitivity to hyper-parameter settings. To address this issue, we propose a new concept of robust policy gradient (RPG). Driven by RPG, this paper further develops a new policy ensemble gradient (PEG) algorithm for DRL, inspired by the recent success of several ensemble DRL algorithms. PEG efficiently and effectively estimates RPG by using multiple policy gradients obtained respectively from several off-policy base learners in an ensemble. The estimated RPG is then utilized for training all base learners simultaneously. Comprehensive experiments have been performed on six Mujoco benchmark problems. Compared to four state-of-the-art off-policy algorithms and four cutting-edge ensemble policy gradient algorithms, our new PEG algorithm achieved highly competitive stability, performance and sample efficiency. Further analysis shows that PEG is insensitive to varied hyper-parameter settings, confirming the positive role of RPG in building reliable and effective off-policy DRL algorithms.},
  keywords = {Deep reinforcement learning,Policy ensemble gradient,Robust policy gradient},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/CTHQW5XR/Liu et al. - 2023 - Policy ensemble gradient for continuous control problems in deep reinforcement learning.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/ZNNKSMM4/S0925231223005040.html}
}

@article{mccarthyWHATARTIFICIALINTELLIGENCE2007,
  title = {{{WHAT IS ARTIFICIAL INTELLIGENCE}}?},
  author = {McCarthy, John},
  year = {2007},
  month = nov,
  abstract = {This article for the layman answers basic questions about artificial intelligence. The opinions expressed here are not all consensus opinion among researchers in AI.},
  langid = {english},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/XQSTB5WU/McCarthy - WHAT IS ARTIFICIAL INTELLIGENCE.pdf}
}

@misc{mnihAsynchronousMethodsDeep2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  year = {2016},
  month = jun,
  number = {arXiv:1602.01783},
  eprint = {1602.01783},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1602.01783},
  urldate = {2025-03-12},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/VQCQQ3W8/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/93X5S7IX/1602.html}
}

@article{mnihHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  urldate = {2025-03-05},
  abstract = {An artificial agent is developed that learns to play~a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a~performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  copyright = {2015 Springer Nature Limited},
  langid = {english},
  keywords = {Computer science},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/LJVE5JTV/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf}
}

@misc{mnihPlayingAtariDeep2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  month = dec,
  number = {arXiv:1312.5602},
  eprint = {1312.5602},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1312.5602},
  urldate = {2025-03-02},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/DBSUNWF2/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/XNSUC4AR/1312.html}
}

@misc{openaiDota2Large2019,
  title = {Dota 2 with {{Large Scale Deep Reinforcement Learning}}},
  author = {OpenAI and Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k e}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and J{\'o}zefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique P. d O. and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
  year = {2019},
  month = dec,
  number = {arXiv:1912.06680},
  eprint = {1912.06680},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.06680},
  urldate = {2025-03-17},
  abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/9UCFQMMW/OpenAI et al. - 2019 - Dota 2 with Large Scale Deep Reinforcement Learning.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/85ADBWF4/1912.html}
}

@misc{openaiProximalPolicyOptimization2020,
  title = {Proximal {{Policy Optimization}}},
  author = {{OpenAI}},
  year = {2020},
  month = jan,
  journal = {Spinning Up documentation},
  urldate = {2025-03-12},
  howpublished = {https://spinningup.openai.com/en/latest/algorithms/ppo.html\#},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/AXGZ2GSL/ppo.html}
}

@misc{RapoiClusterDocumentation,
  title = {R{\=a}poi {{Cluster Documentation}}},
  urldate = {2025-06-01},
  howpublished = {https://vuw-research-computing.github.io/raapoi-docs/},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/CP46MMQ5/raapoi-docs.html}
}

@misc{roderickImplementingDeepQNetwork2017,
  title = {Implementing the {{Deep Q-Network}}},
  author = {Roderick, Melrose and MacGlashan, James and Tellex, Stefanie},
  year = {2017},
  month = nov,
  number = {arXiv:1711.07478},
  eprint = {1711.07478},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1711.07478},
  urldate = {2025-03-05},
  abstract = {The Deep Q-Network proposed by Mnih et al. [2015] has become a benchmark and building point for much deep reinforcement learning research. However, replicating results for complex systems is often challenging since original scientific publications are not always able to describe in detail every important parameter setting and software engineering solution. In this paper, we present results from our work reproducing the results of the DQN paper. We highlight key areas in the implementation that were not covered in great detail in the original paper to make it easier for researchers to replicate these results, including termination conditions and gradient descent algorithms. Finally, we discuss methods for improving the computational performance and provide our own implementation that is designed to work with a range of domains, and not just the original Arcade Learning Environment [Bellemare et al., 2013].},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/78TIBVK3/Roderick et al. - 2017 - Implementing the Deep Q-Network.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/M3H8SNSY/1711.html}
}

@misc{SamsungLabsTqc_pytorchImplementation,
  title = {{{SamsungLabs}}/Tqc\_pytorch: {{Implementation}} of {{Truncated Quantile Critics}} Method for Continuous Reinforcement Learning. {{https://bayesgroup.github.io/tqc/}}},
  urldate = {2025-04-03},
  howpublished = {https://github.com/SamsungLabs/tqc\_pytorch},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/DNC7S9M2/tqc_pytorch.html}
}

@misc{schulmanHighDimensionalContinuousControl2015,
  title = {High-{{Dimensional Continuous Control Using Generalized Advantage Estimation}}},
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  year = {2015},
  month = jun,
  number = {arXiv:1506.02438},
  eprint = {1506.02438},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1506.02438},
  urldate = {2025-03-12},
  abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Computer Science - Systems and Control},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/9UQGZHYL/Schulman et al. - 2018 - High-Dimensional Continuous Control Using Generalized Advantage Estimation.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/B39U64B6/1506.html}
}

@misc{schulmanProximalPolicyOptimization2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = {2017},
  month = aug,
  number = {arXiv:1707.06347},
  eprint = {1707.06347},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1707.06347},
  urldate = {2025-03-11},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/SKFAPYF3/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/X5VYUDJG/1707.html}
}

@misc{schulmanTrustRegionPolicy2017,
  title = {Trust {{Region Policy Optimization}}},
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  year = {2017},
  month = apr,
  number = {arXiv:1502.05477},
  eprint = {1502.05477},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1502.05477},
  urldate = {2025-03-12},
  abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/EYQPF5PH/Schulman et al. - 2017 - Trust Region Policy Optimization.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/7L7GWTPW/1502.html}
}

@article{silverRewardEnough2021,
  title = {Reward Is Enough},
  author = {Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard S.},
  year = {2021},
  month = oct,
  journal = {Artificial Intelligence},
  volume = {299},
  pages = {103535},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2021.103535},
  urldate = {2025-03-04},
  abstract = {In this article we hypothesise that intelligence, and its associated abilities, can be understood as subserving the maximisation of reward. Accordingly, reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence, including knowledge, learning, perception, social intelligence, language, generalisation and imitation. This is in contrast to the view that specialised problem formulations are needed for each ability, based on other signals or objectives. Furthermore, we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities, and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence.},
  keywords = {Artificial general intelligence,Artificial intelligence,Reinforcement learning,Reward},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/6I8KY6BU/S0004370221000862.html}
}

@misc{smithWalkParkLearning2022,
  title = {A {{Walk}} in the {{Park}}: {{Learning}} to {{Walk}} in 20 {{Minutes With Model-Free Reinforcement Learning}}},
  shorttitle = {A {{Walk}} in the {{Park}}},
  author = {Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
  year = {2022},
  month = aug,
  number = {arXiv:2208.07860},
  eprint = {2208.07860},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.07860},
  urldate = {2025-03-17},
  abstract = {Deep reinforcement learning is a promising approach to learning policies in uncontrolled environments that do not require domain knowledge. Unfortunately, due to sample inefficiency, deep RL applications have primarily focused on simulated environments. In this work, we demonstrate that the recent advancements in machine learning algorithms and libraries combined with a carefully tuned robot controller lead to learning quadruped locomotion in only 20 minutes in the real world. We evaluate our approach on several indoor and outdoor terrains which are known to be challenging for classical model-based controllers. We observe the robot to be able to learn walking gait consistently on all of these terrains. Finally, we evaluate our design decisions in a simulated environment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/VTXUZAFW/Smith et al. - 2022 - A Walk in the Park Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning.pdf;/home/james/snap/zotero-snap/common/Zotero/storage/HDJDRAVR/2208.html}
}

@article{srivastavaDropoutSimpleWay2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  shorttitle = {Dropout},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  journal = {Journal of Machine Learning Research},
  volume = {15},
  number = {56},
  pages = {1929--1958},
  issn = {1533-7928},
  urldate = {2025-04-03},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/9IYK8CMG/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf}
}

@article{stable-baselines3,
  title = {Stable-Baselines3: {{Reliable}} Reinforcement Learning Implementations},
  author = {Raffin, Antonin and Hill, Ashley and Gleave, Adam and Kanervisto, Anssi and Ernestus, Maximilian and Dormann, Noah},
  year = {2021},
  journal = {Journal of Machine Learning Research},
  volume = {22},
  number = {268},
  pages = {1--8}
}

@article{suttonLearningPredictMethods1988,
  title = {Learning to Predict by the Methods of Temporal Differences},
  author = {Sutton, Richard S.},
  year = {1988},
  month = aug,
  journal = {Machine Learning},
  volume = {3},
  number = {1},
  pages = {9--44},
  issn = {1573-0565},
  doi = {10.1007/BF00115009},
  urldate = {2025-03-05},
  abstract = {This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
  langid = {english},
  keywords = {Artificial Intelligence,connectionism,credit assignment,evaluation functions,Incremental learning,prediction},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/E3BTFSG8/Sutton - 1988 - Learning to predict by the methods of temporal differences.pdf}
}

@inproceedings{suttonPolicyGradientMethods1999,
  title = {Policy {{Gradient Methods}} for {{Reinforcement Learning}} with {{Function Approximation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  year = {1999},
  volume = {12},
  publisher = {MIT Press},
  urldate = {2025-03-16},
  abstract = {Function  approximation  is  essential  to  reinforcement  learning,  but  the standard approach of approximating a  value function and deter(cid:173) mining  a  policy  from  it  has so  far  proven theoretically  intractable.  In this paper we explore an alternative approach in which the policy  is explicitly represented by its own function approximator,  indepen(cid:173) dent of the value function,  and is  updated according to the gradient  of expected reward with respect to the policy parameters.  Williams's  REINFORCE method and actor-critic methods are examples of this  approach.  Our  main  new  result  is  to  show  that  the  gradient  can  be  written  in  a  form  suitable  for  estimation  from  experience  aided  by  an  approximate  action-value  or  advantage  function.  Using  this  result,  we  prove for  the first  time that a  version  of policy  iteration  with arbitrary differentiable function approximation is convergent to  a  locally optimal policy.},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/LNNKSYKH/Sutton et al. - 1999 - Policy Gradient Methods for Reinforcement Learning with Function Approximation.pdf}
}

@book{suttonReinforcementLearningSecond2018,
  title = {Reinforcement {{Learning}}, Second Edition: {{An Introduction}}},
  shorttitle = {Reinforcement {{Learning}}, Second Edition},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  month = nov,
  edition = {2nd edition},
  publisher = {Bradford Books},
  abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
  isbn = {978-0-262-03924-6},
  langid = {english},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/CN8F6KMW/Sutton and Barto - 2018 - Reinforcement Learning, second edition An Introduction.pdf}
}

@misc{suttonRewardHypothesis2004,
  title = {The Reward Hypothesis},
  author = {Sutton, Richard},
  year = {2004},
  month = sep,
  urldate = {2025-03-04},
  howpublished = {http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/RFR3XQDU/rewardhypothesis.html}
}

@phdthesis{suttonTemporalCreditAssignment1984,
  title = {Temporal Credit Assignment in Reinforcement Learning},
  author = {Sutton, Richard Stuart},
  year = {1984},
  urldate = {2025-03-12},
  school = {University of Massachusetts Amherst}
}

@misc{thompson1jamesthompson1REDQ2025,
  title = {1jamesthompson1/{{REDQ}}},
  author = {Thompson, James},
  year = {2025},
  month = apr,
  urldate = {2025-04-03},
  abstract = {My copy of the original PyTorch implementation of Randomized Ensembled Double Q-Learning (REDQ) algorithm.},
  copyright = {MIT}
}

@misc{thompson1jamesthompson1Sunrise2025,
  title = {1jamesthompson1/Sunrise},
  author = {Thompson, James},
  year = {2025},
  month = may,
  urldate = {2025-06-01},
  abstract = {SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning}
}

@misc{thompson1jamesthompson1Tqc_pytorch2025,
  title = {1jamesthompson1/Tqc\_pytorch},
  author = {Thompson, James},
  year = {2025},
  month = apr,
  urldate = {2025-04-03},
  abstract = {Implementation of Truncated Quantile Critics method for continuous reinforcement learning. https://bayesgroup.github.io/tqc/},
  copyright = {MIT}
}

@inproceedings{todorov2012mujoco,
  title = {{{MuJoCo}}: {{A}} Physics Engine for Model-Based Control},
  booktitle = {2012 {{IEEE}}/{{RSJ}} International Conference on Intelligent Robots and Systems},
  author = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  year = {2012},
  pages = {5026--5033},
  publisher = {IEEE},
  doi = {10.1109/IROS.2012.6386109}
}

@article{towers2024gymnasium,
  title = {Gymnasium: A Standard Interface for Reinforcement Learning Environments},
  author = {Towers, Mark and Kwiatkowski, Ariel and Terry, Jordan and Balis, John U and De Cola, Gianluca and Deleu, Tristan and Goul{\~a}o, Manuel and Kallinteris, Andreas and Krimmel, Markus and KG, Arjun and others},
  year = {2024},
  journal = {arXiv preprint arXiv:2407.17032},
  eprint = {2407.17032},
  archiveprefix = {arXiv}
}

@misc{watchernyuWatchernyuREDQ2025,
  title = {Watchernyu/{{REDQ}}},
  author = {{watchernyu}},
  year = {2025},
  month = mar,
  urldate = {2025-04-03},
  abstract = {Author's PyTorch implementation of Randomized Ensembled Double Q-Learning (REDQ) algorithm.},
  copyright = {MIT}
}

@phdthesis{watkinsLearningDelayedReward1989,
  title = {Learning from {{Delayed}} Reward},
  author = {Watkins, Chris},
  year = {1989},
  month = may,
  address = {England},
  school = {King's College London},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/K6JU4HFV/Watkins - 1989 - Learning from Delayed reward.pdf}
}

@article{watkinsQlearning1992,
  title = {Q-Learning},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  year = {1992},
  month = may,
  journal = {Machine Learning},
  volume = {8},
  number = {3},
  pages = {279--292},
  issn = {1573-0565},
  doi = {10.1007/BF00992698},
  urldate = {2025-03-05},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
  langid = {english},
  keywords = {Artificial Intelligence,asynchronous dynamic programming,Q-learning,reinforcement learning,temporal differences},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/ZQ3XHW35/Watkins et Dayan - 1992 - Q-learning.pdf}
}

@article{williamsSimpleStatisticalGradientfollowing1992,
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author = {Williams, Ronald J.},
  year = {1992},
  month = may,
  journal = {Machine Learning},
  volume = {8},
  number = {3},
  pages = {229--256},
  issn = {1573-0565},
  doi = {10.1007/BF00992696},
  urldate = {2025-03-16},
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  langid = {english},
  keywords = {Artificial Intelligence,connectionist networks,gradient descent,mathematical analysis,Reinforcement learning},
  file = {/home/james/snap/zotero-snap/common/Zotero/storage/P4KGF3SM/Williams - 1992 - Simple statistical gradient-following algorithms for connectionist reinforcement learning.pdf}
}
